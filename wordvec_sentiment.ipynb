{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Word Vectors and Sentiment Analysis\n",
    "CS 224D Assignment 1  \n",
    "Spring 2015\n",
    "\n",
    "*Complete and hand in this completed worksheet (including its outputs and any supporting code outside of the worksheet) with your assignment submission. For more details see the [assignments page](http://cs224d.stanford.edu/assignment1) on the course website.*\n",
    "\n",
    "In this assignment, we will walk you through the process of implementing \n",
    "\n",
    "- A softmax function\n",
    "- A simple neural network\n",
    "- Back propagation\n",
    "- Word2vec models\n",
    "\n",
    "and training your own word vectors with stochastic gradient descent (SGD) for a sentiment analysis task. Please make sure to finish the corresponding problems in the problem set PDF when instructed by the worksheet.\n",
    "\n",
    "The purpose of this assignment is to familiarize you with basic knowledge about neural networks and machine learning, including optimization and cross-validation, and help you gain proficiency in writing efficient, vectorized code.\n",
    "\n",
    "** Please don't add or remove any code cells, as it might break our automatic grading system and affect your grade. **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Honor Code:** I hereby agree to abide the Stanford Honor Code and that of the Computer Science Department, promise that the submitted assignment is my own work, and understand that my code is subject to plagiarism test.\n",
    "\n",
    "**Signature**: *(double click on this block and type your name here)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Run some setup code for this notebook. Don't modify anything in this cell.\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "from cs224d.data_utils import *\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# This is a bit of magic to make matplotlib figures appear inline in the notebook\n",
    "# rather than in a new window.\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# Some more magic so that the notebook will reload external python modules;\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Softmax\n",
    "*Please answer the first first complementary problem before starting this part.*\n",
    "\n",
    "Given an input matrix of *N* rows and *d* columns, compute the softmax prediction for each row. That is, when the input is\n",
    "\n",
    "    [[1,2],\n",
    "    [3,4]]\n",
    "    \n",
    "the output of your functions should be\n",
    "\n",
    "    [[0.2689, 0.7311],\n",
    "    [0.2689, 0.7311]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\" Softmax function \"\"\"\n",
    "    ###################################################################\n",
    "    # Compute the softmax function for the input here.                #\n",
    "    # It is crucial that this function is optimized for speed because #\n",
    "    # it will be used frequently in later code.                       #\n",
    "    # You might find numpy functions np.exp, np.sum, np.reshape,      #\n",
    "    # np.max, and numpy broadcasting useful for this task. (numpy     #\n",
    "    # broadcasting documentation:                                     #\n",
    "    # http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)  #\n",
    "    # You should also make sure that your code works for one          #\n",
    "    # dimensional inputs (treat the vector as a row), you might find  #\n",
    "    # it helpful for your later problems.                             #\n",
    "    ###################################################################\n",
    "    \n",
    "    ### YOUR CODE HERE\n",
    "    if len(np.shape(x)) <= 1:\n",
    "        x = x - np.max(x)\n",
    "        x = np.exp(x)\n",
    "        return x / np.sum(x)\n",
    "    else:\n",
    "        x = x-np.max(x, -1)[:, np.newaxis]\n",
    "        x = np.exp(x)\n",
    "        x = x/np.sum(x, axis=1,keepdims=True)\n",
    "    \n",
    "    ### END YOUR CODE\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== For autograder ===\n",
      "[[ 0.26894142  0.73105858]\n",
      " [ 0.26894142  0.73105858]]\n",
      "[[ 0.73105858  0.26894142]]\n"
     ]
    }
   ],
   "source": [
    "# Verify your softmax implementation\n",
    "\n",
    "print \"=== For autograder ===\"\n",
    "print softmax(np.array([[1001,1002],[3,4]]))\n",
    "print softmax(np.array([[-1001,-1002]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Neural network basics\n",
    "\n",
    "*Please answer the second complementary question before starting this part.*\n",
    "\n",
    "In this part, you're going to implement\n",
    "\n",
    "* A sigmoid activation function and its gradient\n",
    "* A forward propagation for a simple neural network with cross-entropy cost\n",
    "* A backward propagation algorithm to compute gradients for the parameters\n",
    "* Gradient / derivative check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\" Sigmoid function \"\"\"\n",
    "    ###################################################################\n",
    "    # Compute the sigmoid function for the input here.                #\n",
    "    ###################################################################\n",
    "    \n",
    "    ### YOUR CODE HERE\n",
    "    x = 1/(1+np.exp(-1*x))\n",
    "    ### END YOUR CODE\n",
    "    \n",
    "    return x\n",
    "\n",
    "def sigmoid_grad(f):\n",
    "    \"\"\" Sigmoid gradient function \"\"\"\n",
    "    ###################################################################\n",
    "    # Compute the gradient for the sigmoid function here. Note that   #\n",
    "    # for this implementation, the input f should be the sigmoid      #\n",
    "    # function value of your original input x.                        #\n",
    "    ###################################################################\n",
    "    \n",
    "    ### YOUR CODE HERE\n",
    "    f = f*(1-f)\n",
    "    ### END YOUR CODE\n",
    "    \n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== For autograder ===\n",
      "[[ 0.73105858  0.88079708]\n",
      " [ 0.26894142  0.11920292]]\n",
      "[[ 0.19661193  0.10499359]\n",
      " [ 0.19661193  0.10499359]]\n"
     ]
    }
   ],
   "source": [
    "# Check your sigmoid implementation\n",
    "x = np.array([[1, 2], [-1, -2]])\n",
    "f = sigmoid(x)\n",
    "g = sigmoid_grad(f)\n",
    "print \"=== For autograder ===\"\n",
    "print f\n",
    "print g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, use the functions you just implemented, fill in the following functions to implement a neural network with one sigmoid hidden layer. You might find the handout and your answers to the second complementary problem helpful for this part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# First implement a gradient checker by filling in the following functions\n",
    "def gradcheck_naive(f, x):\n",
    "    \"\"\" \n",
    "    Gradient check for a function f \n",
    "    - f should be a function that takes a single argument and outputs the cost and its gradients\n",
    "    - x is the point (numpy array) to check the gradient at\n",
    "    \"\"\" \n",
    "\n",
    "    rndstate = random.getstate()\n",
    "    random.setstate(rndstate)  \n",
    "    fx, grad = f(x) # Evaluate function value at original point\n",
    "    h = 1e-4\n",
    "\n",
    "    # Iterate over all indexes in x\n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        ix = it.multi_index\n",
    "    \n",
    "        ### YOUR CODE HERE: try modifying x[ix] with h defined above to compute numerical gradients\n",
    "        ### make sure you call random.setstate(rndstate) before calling f(x) each time, this will make it \n",
    "        ### possible to test cost functions with built in randomness later\n",
    "    \n",
    "        old = x[ix]\n",
    "        x[ix] = old-h\n",
    "        random.setstate(rndstate)  \n",
    "        fxminus,gradminus = f(x)\n",
    "        x[ix] = old+h\n",
    "        random.setstate(rndstate)\n",
    "        fxplus,gradplus = f(x)\n",
    "        numgrad = (fxplus-fxminus)/(2*h)\n",
    "        x[ix]=old\n",
    "        \n",
    "        ### END YOUR CODE\n",
    "\n",
    "        # Compare gradients\n",
    "        reldiff = abs(numgrad - grad[ix]) / max(1, abs(numgrad), abs(grad[ix]))\n",
    "        if reldiff > 1e-5:\n",
    "            print \"Gradient check failed.\"\n",
    "            print \"First gradient error found at index %s\" % str(ix)\n",
    "            print \"Your gradient: %f \\t Numerical gradient: %f\" % (grad[ix], numgrad)\n",
    "            return\n",
    "    \n",
    "        it.iternext() # Step to next dimension\n",
    "\n",
    "    print \"Gradient check passed!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== For autograder ===\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# Sanity check for the gradient checker\n",
    "quad = lambda x: (np.sum(x ** 2), x * 2)\n",
    "\n",
    "print \"=== For autograder ===\"\n",
    "gradcheck_naive(quad, np.array(123.456))      # scalar test\n",
    "gradcheck_naive(quad, np.random.randn(3,))    # 1-D test\n",
    "gradcheck_naive(quad, np.random.randn(4,5))   # 2-D test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set up fake data and parameters for the neural network\n",
    "N = 20\n",
    "dimensions = [10, 5, 10]\n",
    "data = np.random.randn(N, dimensions[0])   # each row will be a datum\n",
    "labels = np.zeros((N, dimensions[2]))\n",
    "for i in xrange(N):\n",
    "    labels[i,random.randint(0,dimensions[2]-1)] = 1\n",
    "\n",
    "params = np.random.randn((dimensions[0] + 1) * dimensions[1] + (dimensions[1] + 1) * dimensions[2], )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def forward_backward_prop(data, labels, params):\n",
    "    \"\"\" Forward and backward propagation for a two-layer sigmoidal network \"\"\"\n",
    "    ###################################################################\n",
    "    # Compute the forward propagation and for the cross entropy cost, #\n",
    "    # and backward propagation for the gradients for all parameters.  #\n",
    "    ###################################################################\n",
    "    \n",
    "    ### Unpack network parameters (do not modify)\n",
    "    t = 0\n",
    "    W1 = np.reshape(params[t:t+dimensions[0]*dimensions[1]], (dimensions[0], dimensions[1]))\n",
    "    t += dimensions[0]*dimensions[1]\n",
    "    b1 = np.reshape(params[t:t+dimensions[1]], (1, dimensions[1]))\n",
    "    t += dimensions[1]\n",
    "    W2 = np.reshape(params[t:t+dimensions[1]*dimensions[2]], (dimensions[1], dimensions[2]))\n",
    "    t += dimensions[1]*dimensions[2]\n",
    "    b2 = np.reshape(params[t:t+dimensions[2]], (1, dimensions[2]))\n",
    "    \n",
    "    ### YOUR CODE HERE: forward propagation\n",
    "    \n",
    "    #W1 is 10x5\n",
    "    #W2 is 5x10\n",
    "    #data is 20x10\n",
    "    a1 = data\n",
    "    z2 = np.dot(a1,W1) + b1\n",
    "    a2 = sigmoid(z2)\n",
    "    z3 = np.dot(a2,W2) + b2\n",
    "    \n",
    "    #Apply softmax in the last layer\n",
    "    a3 = softmax(z3)\n",
    "    #Cost is negative sum of actual prob * log of computed prop\n",
    "    cost = np.sum(-1*labels*np.log(a3))\n",
    "\n",
    "    \n",
    "    ### END YOUR CODE\n",
    "    \n",
    "    ### YOUR CODE HERE: backward propagation\n",
    "\n",
    "    delta3 = a3-labels\n",
    "    gradW2 = np.dot(a2.T,delta3)\n",
    "    gradb2 = np.sum(delta3, axis=0)\n",
    "    delta2 = np.multiply(sigmoid_grad(a2),np.dot(delta3,W2.T))\n",
    "    gradW1 = np.dot(a1.T,delta2)\n",
    "    gradb1 = np.sum(delta2, axis=0)\n",
    "    \n",
    "    ### END YOUR CODE\n",
    "    \n",
    "    ### Stack gradients (do not modify)\n",
    "    grad = np.concatenate((gradW1.flatten(), gradb1.flatten(), gradW2.flatten(), gradb2.flatten()))\n",
    "    \n",
    "    return cost, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== For autograder ===\n",
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# Perform gradcheck on your neural network\n",
    "print \"=== For autograder ===\"\n",
    "gradcheck_naive(lambda params: forward_backward_prop(data, labels, params), params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Word2vec\n",
    "\n",
    "*Please answer the third complementary problem before starting this part.*\n",
    "\n",
    "In this part you will implement the `word2vec` models and train your own word vectors with stochastic gradient descent (SGD)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Implement your skip-gram and CBOW models here\n",
    "\n",
    "# Interface to the dataset for negative sampling\n",
    "dataset = type('dummy', (), {})()\n",
    "def dummySampleTokenIdx():\n",
    "    return random.randint(0, 4)\n",
    "def getRandomContext(C):\n",
    "    tokens = [\"a\", \"b\", \"c\", \"d\", \"e\"]\n",
    "    return tokens[random.randint(0,4)], [tokens[random.randint(0,4)] for i in xrange(2*C)]\n",
    "dataset.sampleTokenIdx = dummySampleTokenIdx\n",
    "dataset.getRandomContext = getRandomContext\n",
    "\n",
    "def softmaxCostAndGradient(predicted, target, outputVectors):\n",
    "    \"\"\" Softmax cost function for word2vec models \"\"\"\n",
    "    ###################################################################\n",
    "    # Implement the cost and gradients for one predicted word vector  #\n",
    "    # and one target word vector as a building block for word2vec     #\n",
    "    # models, assuming the softmax prediction function and cross      #\n",
    "    # entropy loss.                                                   #\n",
    "    # Inputs:                                                         #\n",
    "    #   - predicted: numpy ndarray, predicted word vector (\\hat{r} in #\n",
    "    #           the written component)                                #\n",
    "    #   - target: integer, the index of the target word               #\n",
    "    #   - outputVectors: \"output\" vectors for all tokens              #\n",
    "    # Outputs:                                                        #\n",
    "    #   - cost: cross entropy cost for the softmax word prediction    #\n",
    "    #   - gradPred: the gradient with respect to the predicted word   #\n",
    "    #           vector                                                #\n",
    "    #   - grad: the gradient with respect to all the other word       # \n",
    "    #           vectors                                               #\n",
    "    # We will not provide starter code for this function, but feel    #\n",
    "    # free to reference the code you previously wrote for this        #\n",
    "    # assignment!                                                     #\n",
    "    ###################################################################\n",
    "    \n",
    "    ### YOUR CODE HERE\n",
    "    vc = predicted\n",
    "    D = len(predicted)\n",
    "    soft = softmax(np.dot(vc,outputVectors.T))\n",
    "    cost = -1*np.log(soft[target])\n",
    "    \n",
    "    gradPred = -outputVectors[target,:] + np.dot(soft,outputVectors)\n",
    "    grad = np.tile(soft,(D,1)).T * predicted\n",
    "    grad[target,:] = grad[target,:] - predicted\n",
    "    \n",
    "    ### END YOUR CODE\n",
    "    \n",
    "    return cost, gradPred, grad\n",
    "\n",
    "def negSamplingCostAndGradient(predicted, target, outputVectors, K=10):\n",
    "    \"\"\" Negative sampling cost function for word2vec models \"\"\"\n",
    "    ###################################################################\n",
    "    # Implement the cost and gradients for one predicted word vector  #\n",
    "    # and one target word vector as a building block for word2vec     #\n",
    "    # models, using the negative sampling technique. K is the sample  #\n",
    "    # size. You might want to use dataset.sampleTokenIdx() to sample  #\n",
    "    # a random word index.                                            #\n",
    "    # Input/Output Specifications: same as softmaxCostAndGradient     #\n",
    "    # We will not provide starter code for this function, but feel    #\n",
    "    # free to reference the code you previously wrote for this        #\n",
    "    # assignment!                                                     #\n",
    "    ###################################################################\n",
    "    \n",
    "    ### YOUR CODE HERE\n",
    "    negativesamples = [dataset.sampleTokenIdx() for i in range(K)]\n",
    "    param = sigmoid(np.dot(predicted,outputVectors.T))\n",
    "    cost = -np.log(param[target]) - np.sum(np.log(1-param[negativesamples]))\n",
    "    \n",
    "    gradPred = outputVectors[target,:] * (param[target]-1)\n",
    "    gradPred = gradPred + np.dot(param[negativesamples],outputVectors[negativesamples])\n",
    "    \n",
    "    grad = np.zeros(np.shape(outputVectors))\n",
    "    grad[target,:] = predicted * (param[target]-1)\n",
    "    \n",
    "    for i in negativesamples:\n",
    "        grad[i,:] += predicted*(param[i]) \n",
    "    \n",
    "    ### END YOUR CODE\n",
    "    \n",
    "    return cost, gradPred, grad\n",
    "\n",
    "def skipgram(currentWord, C, contextWords, tokens, inputVectors, outputVectors, word2vecCostAndGradient = softmaxCostAndGradient):\n",
    "    \"\"\" Skip-gram model in word2vec \"\"\"\n",
    "    ###################################################################\n",
    "    # Implement the skip-gram model in this function.                 #         \n",
    "    # Inputs:                                                         #\n",
    "    #   - currrentWord: a string of the current center word           #\n",
    "    #   - C: integer, context size                                    #\n",
    "    #   - contextWords: list of no more than 2*C strings, the context #\n",
    "    #             words                                               #\n",
    "    #   - tokens: a dictionary that maps words to their indices in    #\n",
    "    #             the word vector list                                #\n",
    "    #   - inputVectors: \"input\" word vectors for all tokens           #\n",
    "    #   - outputVectors: \"output\" word vectors for all tokens         #\n",
    "    #   - word2vecCostAndGradient: the cost and gradient function for #\n",
    "    #             a prediction vector given the target word vectors,  #\n",
    "    #             could be one of the two cost functions you          #\n",
    "    #             implemented above                                   #\n",
    "    # Outputs:                                                        #\n",
    "    #   - cost: the cost function value for the skip-gram model       #\n",
    "    #   - grad: the gradient with respect to the word vectors         #\n",
    "    # We will not provide starter code for this function, but feel    #\n",
    "    # free to reference the code you previously wrote for this        #\n",
    "    # assignment!                                                     #\n",
    "    ###################################################################\n",
    "    \n",
    "    ### YOUR CODE HERE\n",
    "    vc = inputVectors[tokens[currentWord],:]\n",
    "    cost = 0\n",
    "    gradIn = np.zeros(np.shape(inputVectors))\n",
    "    gradOut = np.zeros(np.shape(outputVectors))\n",
    "    \n",
    "    for c in contextWords:\n",
    "        rescost,resgradPred,resgrad = word2vecCostAndGradient(vc,tokens[c],outputVectors)\n",
    "        cost += rescost\n",
    "        gradIn[tokens[currentWord],:] += resgradPred\n",
    "        gradOut += resgrad\n",
    "    \n",
    "    ### END YOUR CODE\n",
    "    \n",
    "    return cost, gradIn, gradOut\n",
    "\n",
    "def cbow(currentWord, C, contextWords, tokens, inputVectors, outputVectors, word2vecCostAndGradient = softmaxCostAndGradient):\n",
    "    \"\"\" CBOW model in word2vec \"\"\"\n",
    "    ###################################################################\n",
    "    # Implement the continuous bag-of-words model in this function.   #         \n",
    "    # Input/Output specifications: same as the skip-gram model        #\n",
    "    # We will not provide starter code for this function, but feel    #\n",
    "    # free to reference the code you previously wrote for this        #\n",
    "    # assignment!                                                     #\n",
    "    ###################################################################\n",
    "    \n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    ### END YOUR CODE\n",
    "    \n",
    "    return cost, gradIn, gradOut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== For autograder ===\n",
      "[[ 0.6         0.8       ]\n",
      " [ 0.4472136   0.89442719]]\n"
     ]
    }
   ],
   "source": [
    "# Implement a function that normalizes each row of a matrix to have unit length\n",
    "def normalizeRows(x):\n",
    "    \"\"\" Row normalization function \"\"\"\n",
    "    \n",
    "    ### YOUR CODE HERE\n",
    "    denom = np.sqrt(np.sum(x**2, axis=1, keepdims=True))    \n",
    "    x = x/denom\n",
    "    \n",
    "    \n",
    "    ### END YOUR CODE\n",
    "    \n",
    "    return x\n",
    "\n",
    "# Test this function\n",
    "print \"=== For autograder ===\"\n",
    "print normalizeRows(np.array([[3.0,4.0],[1, 2]]))  # the result should be [[0.6, 0.8], [0.4472, 0.8944]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Gradient check for skip-gram ====\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "\n",
      "=== For autograder ===\n",
      "(11.166109001533981, array([[ 0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        ],\n",
      "       [-1.26947339, -1.36873189,  2.45158957],\n",
      "       [ 0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        ]]), array([[-0.41045956,  0.18834851,  1.43272264],\n",
      "       [ 0.38202831, -0.17530219, -1.33348241],\n",
      "       [ 0.07009355, -0.03216399, -0.24466386],\n",
      "       [ 0.09472154, -0.04346509, -0.33062865],\n",
      "       [-0.13638384,  0.06258276,  0.47605228]]))\n",
      "(16.514833617181822, array([[ 0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        ],\n",
      "       [-4.91591439, -2.01855734,  1.12480697],\n",
      "       [ 0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        ]]), array([[-0.88442555,  0.40583834,  3.08711651],\n",
      "       [-0.12708467,  0.05831563,  0.44359323],\n",
      "       [-0.45528438,  0.20891737,  1.58918512],\n",
      "       [-0.42136814,  0.19335415,  1.47079939],\n",
      "       [-0.64496237,  0.29595533,  2.25126239]]))\n"
     ]
    }
   ],
   "source": [
    "# Gradient check!\n",
    "\n",
    "def word2vec_sgd_wrapper(word2vecModel, tokens, wordVectors, dataset, C, word2vecCostAndGradient = softmaxCostAndGradient):\n",
    "    batchsize = 50\n",
    "    cost = 0.0\n",
    "    grad = np.zeros(wordVectors.shape)\n",
    "    N = wordVectors.shape[0]\n",
    "    inputVectors = wordVectors[:N/2,:]\n",
    "    outputVectors = wordVectors[N/2:,:]\n",
    "    for i in xrange(batchsize):\n",
    "        C1 = random.randint(1,C)\n",
    "        centerword, context = dataset.getRandomContext(C1)\n",
    "        \n",
    "        if word2vecModel == skipgram:\n",
    "            denom = 1\n",
    "        else:\n",
    "            denom = 1\n",
    "        \n",
    "        c, gin, gout = word2vecModel(centerword, C1, context, tokens, inputVectors, outputVectors, word2vecCostAndGradient)\n",
    "        cost += c / batchsize / denom\n",
    "        grad[:N/2, :] += gin / batchsize / denom\n",
    "        grad[N/2:, :] += gout / batchsize / denom\n",
    "        \n",
    "    return cost, grad\n",
    "\n",
    "random.seed(31415)\n",
    "np.random.seed(9265)\n",
    "dummy_vectors = normalizeRows(np.random.randn(10,3))\n",
    "dummy_tokens = dict([(\"a\",0), (\"b\",1), (\"c\",2),(\"d\",3),(\"e\",4)])\n",
    "print \"==== Gradient check for skip-gram ====\"\n",
    "gradcheck_naive(lambda vec: word2vec_sgd_wrapper(skipgram, dummy_tokens, vec, dataset, 5), dummy_vectors)\n",
    "gradcheck_naive(lambda vec: word2vec_sgd_wrapper(skipgram, dummy_tokens, vec, dataset, 5, negSamplingCostAndGradient), dummy_vectors)\n",
    "#print \"\\n==== Gradient check for CBOW      ====\"\n",
    "#gradcheck_naive(lambda vec: word2vec_sgd_wrapper(cbow, dummy_tokens, vec, dataset, 5), dummy_vectors)\n",
    "#gradcheck_naive(lambda vec: word2vec_sgd_wrapper(cbow, dummy_tokens, vec, dataset, 5, negSamplingCostAndGradient), dummy_vectors)\n",
    "\n",
    "print \"\\n=== For autograder ===\"\n",
    "print skipgram(\"c\", 3, [\"a\", \"b\", \"e\", \"d\", \"b\", \"c\"], dummy_tokens, dummy_vectors[:5,:], dummy_vectors[5:,:])\n",
    "print skipgram(\"c\", 1, [\"a\", \"b\"], dummy_tokens, dummy_vectors[:5,:], dummy_vectors[5:,:], negSamplingCostAndGradient)\n",
    "#print cbow(\"a\", 2, [\"a\", \"b\", \"c\", \"a\"], dummy_tokens, dummy_vectors[:5,:], dummy_vectors[5:,:])\n",
    "#print cbow(\"a\", 2, [\"a\", \"b\", \"a\", \"c\"], dummy_tokens, dummy_vectors[:5,:], dummy_vectors[5:,:], negSamplingCostAndGradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now, implement SGD\n",
    "\n",
    "# Save parameters every a few SGD iterations as fail-safe\n",
    "SAVE_PARAMS_EVERY = 1000\n",
    "\n",
    "import glob\n",
    "import os.path as op\n",
    "import cPickle as pickle\n",
    "\n",
    "def load_saved_params():\n",
    "    \"\"\" A helper function that loads previously saved parameters and resets iteration start \"\"\"\n",
    "    st = 0\n",
    "    for f in glob.glob(\"saved_params_*.npy\"):\n",
    "        iter = int(op.splitext(op.basename(f))[0].split(\"_\")[2])\n",
    "        if (iter > st):\n",
    "            st = iter\n",
    "            \n",
    "    if st > 0:\n",
    "        with open(\"saved_params_%d.npy\" % st, \"r\") as f:\n",
    "            params = pickle.load(f)\n",
    "            state = pickle.load(f)\n",
    "        return st, params, state\n",
    "    else:\n",
    "        return st, None, None\n",
    "    \n",
    "def save_params(iter, params):\n",
    "    with open(\"saved_params_%d.npy\" % iter, \"w\") as f:\n",
    "        pickle.dump(params, f)\n",
    "        pickle.dump(random.getstate(), f)\n",
    "\n",
    "def sgd(f, x0, step, iterations, postprocessing = None, useSaved = False, PRINT_EVERY=10):\n",
    "    \"\"\" Stochastic Gradient Descent \"\"\"\n",
    "    ###################################################################\n",
    "    # Implement the stochastic gradient descent method in this        #\n",
    "    # function.                                                       #\n",
    "    # Inputs:                                                         #\n",
    "    #   - f: the function to optimize, it should take a single        #\n",
    "    #        argument and yield two outputs, a cost and the gradient  #\n",
    "    #        with respect to the arguments                            #\n",
    "    #   - x0: the initial point to start SGD from                     #\n",
    "    #   - step: the step size for SGD                                 #\n",
    "    #   - iterations: total iterations to run SGD for                 #\n",
    "    #   - postprocessing: postprocessing function for the parameters  #\n",
    "    #        if necessary. In the case of word2vec we will need to    #\n",
    "    #        normalize the word vectors to have unit length.          #\n",
    "    #   - PRINT_EVERY: specifies every how many iterations to output  #\n",
    "    # Output:                                                         #\n",
    "    #   - x: the parameter value after SGD finishes                   #\n",
    "    ###################################################################\n",
    "    \n",
    "    # Anneal learning rate every several iterations\n",
    "    ANNEAL_EVERY = 20000\n",
    "    \n",
    "    if useSaved:\n",
    "        start_iter, oldx, state = load_saved_params()\n",
    "        if start_iter > 0:\n",
    "            x0 = oldx;\n",
    "            step *= 0.5 ** (start_iter / ANNEAL_EVERY)\n",
    "            \n",
    "        if state:\n",
    "            random.setstate(state)\n",
    "    else:\n",
    "        start_iter = 0\n",
    "    \n",
    "    x = x0\n",
    "    \n",
    "    if not postprocessing:\n",
    "        postprocessing = lambda x: x\n",
    "    \n",
    "    expcost = None\n",
    "    \n",
    "    for iter in xrange(start_iter + 1, iterations + 1):\n",
    "        ### YOUR CODE HERE\n",
    "        ### Don't forget to apply the postprocessing after every iteration!\n",
    "        ### You might want to print the progress every few iterations.\n",
    "        cost,grad = f(x)\n",
    "        x = x - step*grad\n",
    "        x = postprocessing(x)\n",
    "        \n",
    "        if(iter%PRINT_EVERY==0):\n",
    "            print iter,  \"cost:\",  cost, \"x=\"  ,x\n",
    "\n",
    "        ### END YOUR CODE\n",
    "        \n",
    "        if iter % SAVE_PARAMS_EVERY == 0 and useSaved:\n",
    "            save_params(iter, x)\n",
    "            \n",
    "        if iter % ANNEAL_EVERY == 0:\n",
    "            step *= 0.5\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Show time! Now we are going to load some real data and train word vectors with everything you just implemented!**\n",
    "\n",
    "We are going to use the Stanford Sentiment Treebank (SST) dataset to train word vectors, and later apply them to a simple sentiment analysis task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load some data and initialize word vectors\n",
    "\n",
    "# Reset the random seed to make sure that everyone gets the same results\n",
    "random.seed(314)\n",
    "dataset = StanfordSentiment()\n",
    "tokens = dataset.tokens()\n",
    "nWords = len(tokens)\n",
    "\n",
    "# We are going to train 10-dimensional vectors for this assignment\n",
    "dimVectors = 10\n",
    "\n",
    "# Context size\n",
    "C = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== For autograder ===\n",
      "[[-0.44794105 -0.22838003  0.06474379  0.57003384  1.01117302 -0.51825476\n",
      "   0.05253877  0.63161442 -0.77494881 -0.12194137]\n",
      " [-0.4654531  -0.17745637  0.09411799  0.50996358  0.95586493 -0.50153703\n",
      "  -0.04415326  0.49267503 -0.81421537 -0.16371559]\n",
      " [-0.31450664 -0.0173244   0.04022146  0.42751788  0.84520185 -0.21633817\n",
      "  -0.01071681  0.39182164 -0.51880982 -0.08802799]\n",
      " [-0.36456439 -0.14379799  0.0140219   0.39694779  0.76518765 -0.30675755\n",
      "  -0.01428084  0.48160015 -0.54512889 -0.06588519]\n",
      " [-0.19272271 -0.01391842 -0.06053415  0.21622061  0.33361676 -0.08008315\n",
      "  -0.05399124  0.14109768 -0.12986941 -0.05782017]\n",
      " [-0.41137185 -0.15851162  0.04329356  0.5295404   0.82675427 -0.32222458\n",
      "  -0.04591807  0.4306304  -0.52739191 -0.09206019]\n",
      " [-0.38939221 -0.15446459  0.05900497  0.52560544  0.98475729 -0.36898899\n",
      "  -0.10933784  0.52424992 -0.66116621 -0.14466437]]\n"
     ]
    }
   ],
   "source": [
    "# Train word vectors (this could take a while!)\n",
    "\n",
    "# Reset the random seed to make sure that everyone gets the same results\n",
    "random.seed(31415)\n",
    "np.random.seed(9265)\n",
    "wordVectors = np.concatenate(((np.random.rand(nWords, dimVectors) - .5) / dimVectors, \n",
    "                              np.zeros((nWords, dimVectors))), axis=0)\n",
    "wordVectors0 = sgd(lambda vec: word2vec_sgd_wrapper(skipgram, tokens, vec, dataset, C, negSamplingCostAndGradient), \n",
    "                   wordVectors, 0.3, 40000, None, True, PRINT_EVERY=10)\n",
    "# sanity check: cost at convergence should be around or below 10\n",
    "\n",
    "# sum the input and output word vectors\n",
    "wordVectors = (wordVectors0[:nWords,:] + wordVectors0[nWords:,:])\n",
    "\n",
    "print \"\\n=== For autograder ===\"\n",
    "checkWords = [\"the\", \"a\", \"an\", \"movie\", \"ordinary\", \"but\", \"and\"]\n",
    "checkIdx = [tokens[word] for word in checkWords]\n",
    "checkVecs = wordVectors[checkIdx, :]\n",
    "print checkVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.15641768269104261, 0.13629094697292635)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": [
       "iVBORw0KGgoAAAANSUhEUgAAAmIAAAHiCAYAAABLDqCjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\n",
       "AAALEgAACxIB0t1+/AAAIABJREFUeJzs3XmcV1X9x/HXmRlmhpkBRQU3EJdQytwwxMzKFdDMfSN3\n",
       "c6vMBbPcQBSXzKXFfuVuaYZpmktRIilmpQiS4MIoaSK4IKgsAwwwM+f3B1/HgZmB7zAz3zPL6/l4\n",
       "8HDu/Z5z7+d+m+TtuefeE2KMSJIkKffyUhcgSZLUWRWkLkCSJLW+0Dv0oYzirDtUUBlnx1mtWJIw\n",
       "iEmS1DmUUcwwFmfdfgylrViNMrw1KUmSlIhBTJIkKRGDmCRJUiIGMUmSpEQMYpIkSYkYxCRJkhIx\n",
       "iEmSJCViEJMkSUrEICZJkpSIQUySpM7sKu7n7/RKXUZnFWKMqWvISgihfRQqSVJb1AXqrTQZgcVA\n",
       "KRBW+6wSWJGDujqwGOPq32o97WqtyWwuqC0IIYyKMY5KXUd75ffXPH5/zeP31zx+f83Tmt9f6B/6\n",
       "1Vtr8gG25X8cww8YXa/DGEpjeZzRGrW0lrb0+5ftAFK7CmKSJKkFHc0b0EAIU844R0ySJCkRg1jr\n",
       "mJC6gHZuQuoC2rkJqQto5yakLqCdm5C6gHZuQuoC2rkJqQtoqnY1Wb+9zBGTJKmtaXCO2Jq0wzli\n",
       "bUm2ucU5YpIkdQYVVDKG0ia1V6tzREySJKmFZZtbnCMmSZKUiEFMkiQpEYOYJElSIgYxSZKkRAxi\n",
       "kiRJiRjEJEmSEjGISZIkJWIQkyRJSsQgJkmSlIhBTJIkKZE2u9Zk6B36UEZx7Y4umQVL26MKKuPs\n",
       "OCt1GZIkqW1ps0GMMopXWSX+RmjSqvFtSVMWWZUkSZ2GtyYlSZISMYhJkiQlYhCTJElKxCDWmNE8\n",
       "kroESZLUsRnEGjOCQ1OXIEmSOra2+9RkY67mTmrYlEgR63EH5zKGUbxBCb+lkn3JYw59+TEzuYwa\n",
       "NqMXl3MW4xnL5kzhZiJdAejLpZzIFK7nAioZDEANG1LIM1zMBYziDUaxLXexO+9xAXl8TBX96cJU\n",
       "LuYcAG5lH+ZwOYHFFDKZKrbgUk5O9dVIkqT2pf2NiA3lfEZwICdxIAs4jYmsD5TQg2cZyT4EFjOT\n",
       "HzKco9mW05jLhQD05yO+zTGM4AB25DvM5CoALuRGRjCEgziSwCdsyV2ZM8Xac1axPYMYwcV8nSr6\n",
       "8hu+xGwK+YDrGMgwRnAg1Wy4Sh9JkqS1aH8jYk9xOn9hCAA1bMrrbAUs43T+AUAh08ljGSVEjmA6\n",
       "V9MbgEUU8AeuYwXbE6immq1XOe5YbmZ9bmMYr9Y7ZwEvsS8fAtCFV6mgN8+yhHze5gDeA2ADHuEj\n",
       "vtValy1Jkjqe9jUidhe7s4yvcCoHMZLBFPAqVRQBVXVaRQLLAegCfBo2x3EG+XzISPbjXA6o/RTg\n",
       "ei4gn3c5hwcbPG9gWZ2tGmJtgA0tc2GSJKkzal9BbDndCSygN8t5iG2oYkDmkzUFopWf1dCNgsyo\n",
       "1m84CsgH4Bb2p5KvcgYjG+zXmD14k2q24K9sBsDHHNzEq5EkSZ1c+7o1eQhPcxcncCVPk8+bFDA5\n",
       "88ma5mat/Kw/v2UqtzOaIyliAmSWS5rH6dSwMf/HWACKGceF3NjoMUNmf1+WsQkXM4kxTGYxXXiJ\n",
       "6BwxSZKUvRBj28wOoX/ot9pak+9yAZsnLKm+crrSn6UA/JirKeQthnNnvXZjKI3lcUauy5MkSWmE\n",
       "EGKMca1TmNrXiFhbM5bjeJCjiBTShZc5gt+lLkmSJLUfBrHmGM4dwB2py5AkSe1T+5qsL0mS1IEY\n",
       "xCRJkhJpu7cmK6hkDKW125WwynZ7UkFl6hIkSVLb02afmlxdtk8fSJIkpZZtbvHWpCRJUiIGMUmS\n",
       "pEQMYpIkSYkYxCRJkhIxiEmSJCViEJMkSUrEICZJkpSIQUySJCkRg5gkSVIiBjFJkqREDGKSJEmJ\n",
       "GMQkSZISMYhJkiQlYhCTJElKpEWCWAhhaAihPITwRgjhRw18vl0I4d8hhMoQwvCm9JUkSeqoQoyx\n",
       "eQcIIQ94A9gXeA+YBBwbYyyv02YjoC9wKPBJjPGmbPvWOUaMMYZmFStJkpQD2eaWghY4127AjBjj\n",
       "zMyJ7wcOAWrDVIxxHjAvhHBQU/tKkqR1F3qHPpRR3OSOFVTG2XFWK5SkOloiiG0O1P0fajYrA1Zr\n",
       "95UkSWtTRjHDWNzkfmMobYVqtJqWCGI5E0IYVWdzQoxxQqJSJEmSaoUQ9gL2amq/lghi7wJb1Nnu\n",
       "ndnX4n1jjKOaWpwkSVJrywwOTfh0O4RweTb9WuKpyUnA50IIfUMIhcCxwGNraF934lpT+0qSJHUY\n",
       "zR4RizFWhxDOBsaxMtjdGWOcHkI4c+XH8bYQwsbAZKAbUBNCOBf4QoyxoqG+za1JkiSpPWj26yty\n",
       "xddXSJLUdKF/6Leuk/VjeZzRCiV1Crl8fYUkSWpvruZOatiUSBHrcQfnMiZ1SZ2RQUySpM5oKOez\n",
       "KwuZSRG/ZSyTGMtAFqQuq7MxiEmS1Bk9xen8hSEA1LAp5WzFQF5KXFWn46LfkiR1NnexO8v4Cqdy\n",
       "ECMZTAGvsoKi1GV1RgYxSZI6m+V0J7CA3iznIbahigGpS+qsDGKSJHU2h/A0UMCVPE05F1PAi6lL\n",
       "6qycIyZJUmezKSu4lBNSlyFHxCRJkpIxiEmSJCViEJMkSUrEICZJkpSIk/UlSerIKqhkDKXr1E+t\n",
       "zkW/JUmSWli2ucVbk5IkSYkYxCRJkhIxiEmSJCViEJMkSUrEICZJkpSIQUySJCkRg5gkSVIivtBV\n",
       "kiS1utA79KGM4lY7QQWVcXac1WrHbyUGMXLwy9FWtNNfUklSB1BGMcNY3GrHX5fVA9oAgxi0/i9H\n",
       "W9FOf0klSeqonCMmSZKUiEFMkiTl3n/oxs84EYC72J2r+U3agtIwiEmSpNx7j/VYyEkARAIQ0xaU\n",
       "hnPEUhnFG4xi2yb1+RXfYB4XksccLuOYRttdwfMMZSiDmN/cMiVJahVTuYQa+jKaJ4AVBJZyDbdS\n",
       "RX+6MJWLOQeA+/gi/2MUkRICH7Mn57EX89IW33IcEUunacl/BfAxw9iCH6wxhK3LsSVJyrWduZo8\n",
       "3mYEQ9icq6hiewYxgov5OlX05Td8iY/J5y2uYl9OZwQH0oM/8BwXpy69JTki1pgbOYs8lnE+d3Md\n",
       "o1jB57mMY7iTPZjDMLozno/5PgDFPMUPuQZYOdJVyh0sZT8CS9mbU9iTj/kLvZnC/xEpoSvj6p1r\n",
       "Cd8k0oWu/I0LuYmxbM5kxtCFKaxgB0r4M1XsxjvcyE8YRzFvsISduIjLALia37Apv+ZUJgIhp9+V\n",
       "JEnNVcBL7MuHAHThVSrozXgWUk1/xnM/44FIHnnMSVtoy3JErDE9mchSdgNgBTsSKWEheXzCIAp5\n",
       "i4+4hCEcyQ/Yn+XsxC0MzvQsoTuTGclginiB5zkOgJcYzfr8hpHsT5fMLxrA7XyVFWzFCL7BxQxm\n",
       "OTtyNwMBqGFLenM3I9mXH/BTCpjK5/geP+RqAIIjX5KkDiKwrM5WDZECIJBPOSMYwgiGMJL9uYzj\n",
       "U5XYGgxijTmIaVSxIy9TCiynkBd5hJ2pZBD5zKeQfzOI+ZQQ6c6fWMCgTM/lnMlTAJQyjSr6AFDF\n",
       "QE7iUQD25Y+15/mEr7OMrzGaJ/gxT1DNNixiawDymM0JTF2lLqOXJKkj2JDFRMqATyfr17c3bxLZ\n",
       "kHsYAMDH5PMg/XJWYw54a7IxG1BNHrMYz9EUMYlSXmMue1BNX7oym0p2aqRnVe1PgWoi+ZmtSEEm\n",
       "RtXU+YWLBNbnZs7l96scZSybA0sarS9QtcovbuwEKwNIkjqOQczn70ziSsYTqCSPubWffXrHpydV\n",
       "bMsZ/JeruJJuQD7rcTswI03RLc8gtiZFvMBCvsPmnM+2lPM0V1DAVLbjJf7BaCayPp9nIQs5lI24\n",
       "Y43HKmASd3Io3+dPPM3htfs3YAIfcCHl/In+LGU8G1PMisynjc/16sZs5nMSK4C/sxlV7NwCVyxJ\n",
       "Uu5ckplrvbofMaL252OZDhyRo4pyzluTa9KDiUR6sg8v8jU+IlBJV55nb+ayEdfwBH/kZ4yjkKmc\n",
       "xfhMr4ZvHu7MSOZzMlfyJCvoVbv/dJ6ljD/xAI9zJeN5jluZX7sU0erH+mz7ZCaRzyyuZQIvcgUF\n",
       "TGuwnSRJarNCjO3j7+wQQowxtsrTgKF/6NdZ1pqM5bHDDOdKktqPVv+7to39HZdtbnFETJIkKRGD\n",
       "mCRJUiIGMUmSpER8alKSJLW+CioZU/swWuscvx1ysj4Qeoc+lLXge7iWswmFbfC9XiuopAsfNOsY\n",
       "FVTG2XFWC1UkSVKHlG1ucUQMaOlgEfoHOuxTmK35XzOSJHUyzhGTJElKxCAmSZKUiEFMkiQpEYNY\n",
       "R3cfO3AdV6QuQ5Ik1edk/Y7uOF4GXk5dhiRJqs8glktXcyc1bEqkiPW4g3MZwyjeoJQ7WMp+BJay\n",
       "N6ewJx9zLTeRRwXL2ZFITzbiKr7LXwH4CZdRyd5ADRvyc77Hn7mWn9GDsZzFOACu4WY24DEKWcT7\n",
       "nMWlnMz1DKeKzalmC2rYjG7cwfncDcANnMcSDiePeeTxPsVMZTi3pfqqJEnqDLw1mUtDOZ8RHMhJ\n",
       "HMgCTmMi6wMldGcyIxlMES/wPMfVtq+mJyM4lC9wEvO4FIBfcSDL+QIj2Ze9OJZ5jGQCG7ExY/iI\n",
       "YwCYShlV7MqJjM8c6bOXxVWxDd/lWPbnIBZyAQvJ4152YikH8B324XCOp4odc/WVSJLUmRnEcukp\n",
       "TudKxvFbHqeGTXmdrYBlnMlTAJQyjSr61Lbvzt8AOJL/UsNGACxiIN15BICv8RGF/JtyduZUJlLN\n",
       "ljxHD57mUIr5CyXUf1tvCePZgGq+zCcE5vICPZnLlyjmCXpSxRdYQhFPtvI3IUmSMIjlzl3szjK+\n",
       "wqkcxEgGU8CrVFEEVNW2CVQTya/dzmN5nSM09nbez/aX8kee4wgWcQzbcH8jreses5oVdc4nSZJy\n",
       "yiCWK8vpTmABvVnOQ2xDFQMyn2S7bNPKdt2ZyEIOZgmBf7IByxnEF/gPAAN5gEWcDkSO4M2sj9mL\n",
       "SVSyP7Mp5DVKWMZ+Tbo2SZK0TgxiuXIITwMFXMnTlHMxBUzOfNLYYp+r71+5/R3+RiHTuYHxPMUf\n",
       "6MlovsZHwMpblfnMoDt/yLKqlcc8nml0ZRx3MZ6HuZcCppPPwqZcniRJajoX/W4FoX/ol2StyRkU\n",
       "M4bxHMYQdmji+cvpSn+W8hbF3MfDfI4LGcar9dqNoTSWxxktVbIkSR2Ri353NnewJ+9yI925pckh\n",
       "DOBhrqeafkSKKOWBBkOYJElqUY6ItYJkI2K54IiYJElrlW1ucY6YJElSIgYxSZKkRJwj1hoqqGQM\n",
       "panLaBUVVKYuQZKkjsI5YpIkSS3MOWKSJEltnEFMkiQpEYOYJElSIgYxSZKkRAxikiRJiRjEJEmS\n",
       "EjGISZIkJWIQkyRJSqRFglgIYWgIoTyE8EYI4UeNtPlFCGFGCOGlEMIudfa/HUKYGkL4TwjhhZao\n",
       "R5IkqT1o9hJHIYQ84JfAvsB7wKQQwqMxxvI6bQ4Atokx9gshDAJ+Deye+bgG2CvG+Elza5EkSWpP\n",
       "WmJEbDdgRoxxZoxxBXA/cMhqbQ4B7gGIMU4E1gshbJz5LLRQHZIkSe1KSwSgzYFZdbZnZ/atqc27\n",
       "ddpE4MkQwqQQwuktUI8kSVK70Oxbky3gKzHG90MIPVkZyKbHGP/ZUMMQwqg6mxNijBNyUaAkSdKa\n",
       "hBD2AvZqar+WCGLvAlvU2e6d2bd6mz4NtYkxvp/559wQwp9YeauzwSAWYxzVAvVKkiS1qMzg0IRP\n",
       "t0MIl2fTryVuTU4CPhdC6BtCKASOBR5brc1jwImZwnYH5scY54QQSkIIZZn9pcBg4JUWqEmSJKnN\n",
       "a/aIWIyxOoRwNjCOlcHuzhjj9BDCmSs/jrfFGMeGEA4MIfwXWAyckum+MfCnEELM1HJfjHFcc2vK\n",
       "Vugd+lBGca7ORwWVcXactfaGkiSpMwgxxtQ1ZCWEEGOMoUWP2T/0YxiLW/KYazSG0lgeZ+TsfJIk\n",
       "KYlsc4uvjZAkSUrEICZJkpSIQUySJCkRg1hruJ7h3MQZqcuQJEltm0FMkiQpkbbwZv2252aO5BPO\n",
       "JFBDAdPZieuZwk3U0IM8PmYg5zOE9xnL5g3ulyRJyoIjYqt7kH58wjkcwJGMYAj7cTn/4SrW4w+M\n",
       "ZDDd+ROTuQqg0f2SJElZMIitbjZ70pXHGcgCAAaygCq+xEk8AsDx/JEqBgI0ul+SJCkLBrHstI+3\n",
       "3kqSpHbFILa63vyTpXyTiawPwETWp4DJ/JZDAfgdR1DARAAKmNTgfkmSpCw4WX91RzGDm/k5T/AQ\n",
       "46iigFfYlcuYzE+5krNqJ+UDDOAyXmxgvyRJUhZca9K1JiVJUgvLNrc4IiYAQu/QhzKKU9fRqAoq\n",
       "4+w4K3UZkiS1JIOYViqjOKejg001htLUJUiS1NKcrC9JkpSIQUySJCmRzn1rsoLKVr3ltZxNKKwz\n",
       "72oFlaF/iz5v0JK2ZApLAIgsY1feS1yPJEkdXqcOYq09+Tv0D7TpeVd1PccSBmSC2BRKABjL5kzm\n",
       "Hkay7zod83ouoAfPcxr/arE6JUnqQDp1EFNW1u39JksIXMiNLVyLJEkdikFMa9OFa7iZKnaggHKO\n",
       "41yeZCDvM4JIHl2YymlcRE+quILn6cpjLOOrbMCvWMjerMeTfJe/cgXPU8KDVLI/kXx24EwO4y3+\n",
       "RQ+e5lfU0IsuTGE5X2MoQxjE/NQXLklSa3Oyvtashm3ozd2MZC8CFTzImbzLT9mJMxjJ/kABv+PE\n",
       "2vb5fMwIDuB7PF7vWF2YxwiG0p17eZ2zAHiWC+jKs4xkXzbkz0Q2y9WlSZKUmiNiuZbtvKu686uu\n",
       "4kG24gqO4xWu4HmGMpRBzGc0jzAis9ZlU/2Co9iBCezN3DW2C7zLiUwBoBcP8z7nkc9MDmYmAD15\n",
       "kA85CbgLgO15rNFj7cBfAdiQaczkAABWMJAdORWAM3iGK1iwTtfTSnL+oltfXCtJnYpBLI01z7ta\n",
       "8/yqz/quawgDWMjRvEc5rCWIrV5rYCExsyB6Q3pkJvw3pIzlAORRDeRnWWlauX7RrS+ulaROxSCW\n",
       "xqrzro7lPO5lQqPzq1b12fsvRvEGo9iWcrryR+4msh6RAnpxPWcxLjP6dh+FvMByvkQe73Myp/AY\n",
       "+1HFTrzJLxlNJafwzUYrjfTmHnbhRP7DhxxGMS9RwQk8zhZ8k3eYyxF05blmfBOTmM4hHMivuJ2v\n",
       "EVlvnY8lSVI74xyxFFafd/UIJwFxjfOrGrZytGozlnE4pzKCA9ifo/mQkXXOtSVbcRcj2Yc8FvII\n",
       "B/JdxlLAVD7H9xjBEHpnRqoaksd/eZeTuZIJRLpzJLfTm/N5idu5kieBGo7n3lXqWb2+hj9baU9u\n",
       "Yilf40rG8xHfIPAhW1OR5fVLktSuOSKWwurzrj7g28Ca51etSRWBx7iEPzKIQA01bMI/2BCAPN7h\n",
       "GMoBKOJlKulT229tL6Y4kHc5kL3q7f82/waG1Nt/OV9eZftiLmjws+N4GTgagG1YxI58i+7UcA8D\n",
       "mMnO9KRqLZVJktQhGMTSaHjkaE3zq9bkdxxODRvwAwZTQuQKnmcxRZlPPxvtClQTa/e3DS+wOdO4\n",
       "lUggsJyt+UHqklpFYw9cSJI6NYNYCqvPu+rKRBax/TocaeV8sWq6kc88SojcyR5EetdrU79nBZV0\n",
       "W4dztqxDeJtDGhhd69jW7SW5kqQOxzliKaw676obh9bOsaqrsflV9X/enT+xnJ24kif5kCPIY0Yj\n",
       "7T/Tgwd4l+sYzRPMpnAdr6TzuZGz+CmnAHAdo7iKPwBwJ3twDTdzO19lNI8ymr9yDb9mRoOvvmiz\n",
       "C45KknLLEbFca2ze1ZrmV12WmU+1ertRbAfAl/mEL3NII+fbr/bn4dxa+/PKpzE/eyLTN1dlpycT\n",
       "mc0ZwN2sYEciXVhIHp8wiCKmM4fzOI5j2JpKbuS7PMqZ/ICfpy5bktQ2GcT0qWU8l1ns+21KeL2N\n",
       "vc+qgsrUJQBwENP4JTvyMqXAcgqZxiPsTCWDKOUJqtmW+3gUgEgXCpmctmBJUltmENNKX+a92p/f\n",
       "pjSWxxlraN15bUA1ecxiPEdTxCRKeY257EE1fSnhHZbyDJdwduoyJUntg3PEpKYq4gUW8h168Dxf\n",
       "5AUqOJECXuGLTGEFA3mMvgDMoJhH2CpxtZKkNswRMampejCRxXyffXiRralkApV05Xm+wieUcx5T\n",
       "+RUvUQhEevET4H9k83JbSVKnE2JsH38nhBBijLFdPW2W8wWjW4oLT9cK/UO/XK816W1hSWr/ss0t\n",
       "joi1IsOMJElaE+eISZIkJWIQkyRJSsQgJkmSlIhzxKQ1qaCSMTl8uW1beXGtJCknfGpSkiSphWWb\n",
       "W7w1KUmSlIhBTJIkKRGDmCRJUiIGMUmSpER8arITa7NLMLnEkiSpkzCIdWZlFOd0HcVs5fJ1EZIk\n",
       "JeStSUmSpEQMYpIkSYkYxCRJkhIxiCn37mJ3fsuuqcuQJCk1J+sr9z5iD/JZDLyYuhRJklIyiGlV\n",
       "MyjmAW6lhk2I5FPKw1SyC5dyOr9mMHP4NeewLUvI5y4mMJI9eJwtmMo1RDYgsJTtuZDDeIt/0YNn\n",
       "uI5qNgNgCy6nJx+whBOAKkZzOH24jJOZlPaiJUlKwyCmVT3J3uTzAZdyEgBTKeNRjgNgEbuRz3Qe\n",
       "Z2eqKaCAKZk2P2FHfsTBzOReduZVruUwjuEZRrMZt3Eyk/kbm/ECv+ck9uJV7iWfCoZzW7LrlCSp\n",
       "DTCIaVWbUc5cRvITLqYn4zmFSTzOTB5iG5azCxtwG/PYnUg+JUyknK5UMZCp3MbUzDEiXQBYwVeZ\n",
       "RT9G1+4vZUYbfIGsJEmJGMS0qkP5H30YwnPsw7v8kBv4F8U8z0z2AVYwgGd5ip8BeWzBaFaQR2A+\n",
       "IxjSwNECZ/ENelKV46uQJKld8KlJrerv9GJjKjmbR+jJLSzji2zARBZxOkVM4st8Qg09qGYbjuYN\n",
       "dmAxecziV3yj9hj383kAuvAM93Ba7f4xfAGAfCqopltuL0ySpLbHETGt6i368y9GEKgBVrANF/EV\n",
       "ZnA3G9KDiQB0YTrVbFTbZ1e+xxSu40rOBQoo4VFgOnszkqe5mit5EsinkOeBS9iaJ5nGbYxmsJP1\n",
       "JUmdWYgxpq4hKyGEGGMMqevoSEL/0K+trjUZy+OM1GVIkrSuss0t3pqUJElKxCAmSZKUiEFMkiQp\n",
       "EYOYJElSIj412ZlVUMkYSlOXUU8FlalLkCQpF3xqUpIkqYVlm1scEZNyLPQOfSjL0VJPFVTG2XFW\n",
       "Ts4lSWoyg5iUa2UU5+z9bW3x1rMkqVaLTNYPIQwNIZSHEN4IIfyokTa/CCHMCCG8FELYuSl9JUmS\n",
       "OqJmB7EQQh7wS2AIsD0wLITQf7U2BwDbxBj7AWcCt2TbV5IkqaNqiRGx3YAZMcaZMcYVwP3AIau1\n",
       "OQS4ByDGOBFYL4SwcZZ9JUmSOqSWCGKbA3UnA8/O7MumTTZ9pY5vLJtzJX/PeV9JUlKpJuuv02so\n",
       "Qgij6mxOiDFOaJFqpLahOe+SaR/voZGkDiqEsBewV1P7tUQQexfYos5278y+1dv0aaBNYRZ9a8UY\n",
       "RzWnUKmN68I13EwVO1BAOcdyHg9zFkvZj0gxhUzmIi4C4D524E1uJBAp4h+J65akTi8zODTh0+0Q\n",
       "wuXZ9GuJW5OTgM+FEPqGEAqBY4HHVmvzGHBiprDdgfkxxjlZ9pU6hxq2oTd3M5K9CCzmEU5kb+5i\n",
       "BAcxkv2IdOVW9gXgTW5iCy5lBEMSVy1JaoZmj4jFGKtDCGcD41gZ7O6MMU4PIZy58uN4W4xxbAjh\n",
       "wBDCf4HFwClr6tvcmqR2KfAuJzIFgF48xAd8m4nMYizfJdKVyHpU8Dr/4QUi3TmZSQD05SFeZ++U\n",
       "pUuS1k2LzBGLMf4N2G61fbeutn12tn2lTmr1eV6RuVzDVxjKfszheoYTKcp85nJfktQBtMgLXSW1\n",
       "gEhv7mEXAD7kMLoyEYD+fMJrlLCUbwCwC4sIzOc3fAmAmRyepmBJUnO5xJHUVuTxX97lZK7kpxRQ\n",
       "zhHcwwOsz108TeBDuvBSbdttGM6b3MRoIkU8k7BqSVIzhBjbx1Pv2a5iLrV1oX/ol8u1JmN5nJGT\n",
       "c0mSamWbWxwRU7sReoc+lFGcuo5aFVTG2XHW2htKktQwg5jajzKKczaSlI0xlKYuQZLUvjlZX5Ik\n",
       "KRGDmCRJUiLempRyrYLKnN3WrKAyJ+eRJK0Tg5g6jusZTj4VDOe2Zh3nKh5kK67gOF5pocpW4QR/\n",
       "SdKnvDUpSZKUiCNiat9u4ByWcCR5zCOP98ln2iojWs/Rg3H8lcvZnV9wFIsYSqSEarakO7cS6cJi\n",
       "jgQqOZAT2JWFAMzmKEZzI5F8tuICTmBq2guVJHVEjoip/bqPL7KEb3Iq+3EIJ1DFTqxcr7H+mo2f\n",
       "qmZbjuEU9uNAFvAj8lnMCIZQyBT+wZF1ehQzgiH04RL+x005uR5JUqdjEFP7NYdBdOVv9GY5O7CY\n",
       "Ip5gbYthF/Jv+lHJV/iEwEK+yHgAulLOcvrUtuvJIwCcwgtEyphKWatdhySp0zKIqSP5NIRVEzO/\n",
       "2xUUrdZieZ2tSFntdg11b9WHVUbVAnn1RtkkSWo2g5jar02YyFKGMJtCXqaUZewPRAqYxTx2AuBl\n",
       "DlqnY3/IwQDczUACC9ihDb3RX5LUYThZX+3Xt3iFG3mMuxhPHvMo4CUAtuMWXuZWRnMcxZlbjw1r\n",
       "bJQrEljGaJ4gks82nN/itUuSBIQY28cdl2xXMVfHFfqHfm1trclYHmekLkOSlF7oHfpQRnHtjrd4\n",
       "g63ZtsHGFVR++k5JR8QkSZKaq4ziVQYLboRGBw/qrK7iHDFJkqREDGKSJEmJeGuyk6t3T7utqHP/\n",
       "XJKkjsog1tmtfk+7rahz/7xWBZUN7k+lgsrUJUiS2jeDmNoNR8gkSR2Nc8QkSZISMYipdf2awTzE\n",
       "NqnLkCQ7eITfAAAbN0lEQVSpLTKIqXXNZyhz2C51GZIktUXOEVN9N3IWeSzjfO7mOkaxgs9zGcdw\n",
       "J3swh2HksYgV7EykiK78hQu5CYCfcAmV7A+soJhn2Ji/sZzBzGN3RnMuu3AakcBUriGyAYGlbM+F\n",
       "HMZbaS9YkqQ0DGKqrycTmc0ZwN2sYEciXVhIHp8wiFKeY0/+zK4sZAmBm3iAB9iOvsyhkqGM5GsA\n",
       "TKWMnajgWsaxHk/yXf4KwFXcz478iIOZyb3szKtcy2Eck/BqJUlKxiCm+g5iGr9kR16mFFhOIdN4\n",
       "hJ2pZBBbcSnPcghjOY5IPpFezGVbhjKDcSzlWm5gfcZzZAOLbZfTlSoGMpXbmJrZF+mS02uTJClX\n",
       "ruYe9uAC9mZuY00MYqpvA6rJYxbjOZoiJlHKa8xlD6rpSzHLWMiZHMLQzIjXTdRQRHdqOItv8Cf2\n",
       "5CO+ya2cwmWrjXStII/AfEYwJNGVSZKUO5dy4tqaOFlfDSviBRbyHXrwPF/kBSo4kQJeYRHdCCzm\n",
       "C1QwgY1Yzj4AzKCY1+nOGUxgKKOo5gsA5LGYKroBsAOLyWMWv+Ibtee5n8/n/uIkSWobHBFTw3ow\n",
       "kcV8n314ka2pZAKVdOV5jmU61/Iq1/IP8niPLrwAwGy68U/u5imKANiAywHYmEd5h+sZzanswuns\n",
       "yveYwnVcyblAASU8CkxPdJWSJCUVYoypa8hKCCHGGEPqOjqa0D/0a6tLHMXyOCN1GZIkZaPe36c3\n",
       "8i4XsHmDjev8HeeImKSstdgi8S7qLkmAQUxSU7TUIvFtafF2SWoJFVSu8u+2Shr/d10FlZ/+aBCT\n",
       "JElqptVH+UMIZDPFxqcmJUmSEnFETFLzLCSPX3Av2zGKo5hRb7uu59gMKAJKQv9Ez944P01SG2IQ\n",
       "6+xWv6fdVtS5f642rjs17MX3eYYbWMhpDW5/pogvs4QiYECip3Xb4u+7pE7LINbJOTKgFrEnH7Mn\n",
       "pza6LUlqkHPEJEmSEjGISep4xrI5V/L31GVI0toYxCR1VO1j2RBJnZpzxCSldwPnsYTDyWMhBcyj\n",
       "K+X04kXe5EIiRRTwLrtxDT1YTDmf460G9r/CVrzNRQQiRfwj9SVJUjYcEZOU1u/YkaUcwHfYhz35\n",
       "IVVsRxEreItL2IEbGcHRdON1XuEEBrCE/zWyfyaXswWXMoIhqS9JkrLliJjatBZb27At6+zvtfqQ\n",
       "gRTzBD2pojtL6cYzVNOVGrpxCC8BsCuP8xTX8Q6lDe5/mzIiZZzMJAD68hCvs3eya5KkLBnE1La1\n",
       "1NqGbZnvtVrdur7pNdEbYiVp3XlrUlJavZhEJfszm0IW0pVFfJV8lpLHQh5lJwBe5BuUMYUtWNzg\n",
       "/i2pILCI3/AlAGZyeLLrkaQmcERMUlNuAW/Jcyyps72ML/Nes05+PNO4gXHcxXjyWUQRM+hCBbsx\n",
       "kklcyisU04XZHM0ogEb3b801vMk1jCZSxDPNqkmScsQgJin7W8BTWMKAOkHsOUpa5PwHcQv9+SnP\n",
       "sh3P8kv68Br7M4P9Oble28b2f543OI7BdfZc0yK1SVIrMoip4/sZx5PPEr7Pwy12zKt4kK24guN4\n",
       "ZZX9v+AolrATF3FZi52rM3iY66mmH1DGRjzKYN5IXZIk5YJBTB3fefwup+cLHfhFopFlTKkzCvY2\n",
       "JbzdpIcNSijKHKeuSzgbgClstcqImyR1cAYxtU83cxjz+TbQhS5M4Rwu4Se8Til3sJT9CCxlb05h\n",
       "Tz7meoaTTwXDuY0xbM9/uZZIMfnM5ECGM5sevMRtjGAoAI+yJdO4hREM5QbOYyn7ESmmkMlcxEW1\n",
       "NczmKEZzI5F8tmY4xzNtlRr/RQ+e4Tqq2QyAPoziZCbn7DtqDbuuNh/sdUpjeZyRbffQP8CAJjwF\n",
       "ex9H8RGHAZDHJ9TQgyJe40yuyvoYktSG+dSk2p+H2IaFHMy5HJx5eWcNd3I40JXuTGYkgyniBZ7n\n",
       "uHp9Z/AzejOakQymkHLGMZxv8g6BBdzP5wF4nWMo434A9uYuRnAQI9mPSFduZd/aY0WKGcEQ+nAJ\n",
       "b/HTeud6htFsxm2M4CAGcgbvcEOrfB8d2XE8yDl8i3P4FmfzPc7hW4YwSR2JI2Jqf97hq1SxAz/n\n",
       "r8DKQFTAPGA5Z/IUAKVMYxFfXaXfVMqIdOeUzEs/+/MgL3ErAOszhpkcwxKuoJKD2ZsDAZjIwfyV\n",
       "E4kUE+nGUj5kCm8RKGZjJjKFrdiJucxifZ5ie/LpSR7dmcJWrGAvZrM9V9W+36o7/6A/ZavdloOS\n",
       "0D+LV2Dl+sWvV/A8l7M7Y9mcKfyMyzgqZ+eWpE7CIKb2KNCVB/kh162ydxRn1WlRTSS/wb4NOZSx\n",
       "3MVw7uHfFDCVgSxgNoXM5UccwjB2Zi6/5AwCgQEs4QlqWJ+ltfOZ/kJkW5Ywk2WsoIoBLOHPwLkc\n",
       "Tzeq1ng1y4AvZ3G7Lvcvfo2N/LzuKqhcy3WU1Iup2XqbEl7P4juqoHIdzyBJLc4gpvanD8/yGnfx\n",
       "T25nTz5mEusxj1LW9mb1najgMT7hNwzkZCZRzpEU8hwAvVlOEc8wh2vZlOEAzM1MK+/LfN6lKwvY\n",
       "j/V5svZ4bzMEmMKj7Ewei+i92iTzEp7jPoZxFvcCMI5t29XTgHl8BEA+NeQxvyUOubYRvdA/ZBdK\n",
       "G/J20+arSVJbYBBT+3Mk/+X/+AlPM4anyCOwgr5cyppHbVZ+9jnO57/8mCspJp93OJDza1tsysP8\n",
       "j6GcknkZ6C4s4mke55f8kQLmUbzKqyoieSzjan4P5DMg81LRug7meh7lIq7mfiL5lDKFwVzb7OvP\n",
       "lREcBMAQ3mcIZySuRpI6JIOY2qfv8Wfgz6vt3a72p+8yFhgLkHnSbuVIzDBeAw5u8Jjz2I0S7qdL\n",
       "nX17cQcD+EW9thdzZoPHOKVOXduygAu5OIurkSR1Uj41qY7ten7Acnbmy4xbY7uruYMKjuBr3Jmj\n",
       "yuobwxe4lb2TnV+SlHOOiKlju5AbIIvXRlzKaWv8fAYbU0NR1uetJpCf5QT3hRTzHLCAvVjCdjzH\n",
       "2420/Ozpylw/QSlJahUGMSkbNRSxHUtrt//EMD5iXwqYTxfm0Y03+JhBlPAWi9meDXmagYzn75zL\n",
       "CnoCsD23sBvTeZF+TOM7QCGwjD78HzvyJs9zKlDEJL7IZtzFkYxfpYYiPnsZau6foMyNtT9Vuea+\n",
       "ktTOGMSkpprC55jPnhzPGSylkIf4P7plnoaM5HMK5wBwHz+iHw8ziNd4i414imvYjTPYmlnsxHAK\n",
       "gH+xM69wPPsxkU35NYv4Aqfzk4RXl5SjfJI6G4OY2rbmjJC0jJXvtVpIce3bwF5lZwKTeDNzqzIw\n",
       "iQV0YTF5bMDzvEpXAOaxC/PpW7uoUTUlvMh6VFDGdE5nBZsSiFQ7V1OSOiuDmNq01CMkte+1eg7Y\n",
       "PvOesCnMJ1LN9rwDwHNUEJlPDcvoxtu1+58ATuNYSqle5aC3cDll/IPj+QMvsQlPc3sur0mS1Hb4\n",
       "X+JSU23OSyziayygCx/QlQq+2uCrZLvyHH/kW7Xbz7AtANWUUcKHAPydy2qn9HdhMe9yZCtXL0lq\n",
       "QxwRk5pqH6Yzk2e4mz+Qz0cUMoMCKuq1G8z1jOMifp55oWsJL/J1fsy23MOrXMHPOY3F9KY003dX\n",
       "JvMyefyC3zc4Wb81restYCfIS1KzNCuIhRB6AH8A+gJvA0fHGBc00G4o8DNWjsDdGWO8LrP/cuB0\n",
       "yIwOwCUxxr81pyYpJ5ZSTQl/4Age4Tb+zHw25RJO5GG+xA1cRS+eZRanAtCNf3IuNwNwBf9kfR5i\n",
       "MctYnwnUcBqVvM+PuYWLOItAJYF/8jqncQ3Hcxjn8fmWWV5oTVLfApakzqq5tyYvAsbHGLcDnoL6\n",
       "bxEPIeQBvwSGANsDw0II/es0uSnGOCDzxxCm9mE5A5jLd7mX+6ihEohUksf77EJXZjKTczia07mQ\n",
       "Y1nC9vyerwMQ6cqGTONShvE9biefDzmW07kos2B5pCs9mcqlHEsZU3iawxNepSSplTU3iB0C/Dbz\n",
       "82+BQxtosxswI8Y4M8a4Arg/0+9Ta16oWWqLzuBcapjPkZxIPrMpYRpPsj2LGEAXFlHCZPqxkEIi\n",
       "vfgrcxmQ6VnNMP5e50iBmlX+P7CcY/kXAD2YTiWb5eqSJEm519wg1ivGOAcgxvgB0KuBNpsDdW97\n",
       "zM7s+9TZIYSXQgh3hBDWa2Y9Um6UUE0X3mcsB1PGS2zEf5jFl1hBb7rxHrGR/8AILCN/DccNtS/J\n",
       "WBnR4hpbS5LaubUGsRDCkyGEaXX+vJz5Z0MLJ2e3pMtnfgVsHWPcGfgAuKmJ/aV0ypjCHE5gU6aw\n",
       "C//hI46kiHK+yKssZQAz6E4leXzIUHrVvk1s1YCWRwUfrzJJ3hFiSepE1jpZP8a4f2OfhRDmhBA2\n",
       "jjHOCSFswmeT7ut6F9iiznbvzD5ijHPr7L8deHxNtYQQRtXZnBBjnLDm6qVm+uxpwpJ6K02W8hrz\n",
       "+Da9mMEylgMrKOYVVrCUntzC/ZkFxEv4N9vxIlMoAaj9J0A3/sI4fs145nIA567y+WIKqaKgdjuy\n",
       "rFWvVZK0zkIIewF7NblfjE0dxFrlpNcBH8cYrwsh/AjoEWO8aLU2+cDrwL7A+8ALwLAY4/QQwiaZ\n",
       "W5qEEM4HBsYYv0UDQggxxuhogZII/UM/hmXWeWwLxlAay+OM1GVIkhqWbW5p7nvErgMeCCGcCswE\n",
       "js6cfFPg9hjjQTHG6hDC2cA4Pnt9xfRM/5+EEHYGalj5+oszm1mPJElSu9GsEbFcckRMKTkiJklq\n",
       "imxzi0scSZIkJWIQkyRJSsS1JqVsrOtajK3FNR4lqUNwjpgkSVILc46YJElSG2cQkyRJSsQgJkmS\n",
       "lIhBTJIkKRGDmCRJUiIGMUmSpEQMYpIkSYkYxCRJkhIxiEmSJCViEJMkSUrEICZJkpSIQUySJCkR\n",
       "g5gkSVIiBjFJkqREDGKSJEmJGMQkSZISMYhJkiQlYhCTJElKxCAmSZKUiEFMkiQpEYOYJElSIgYx\n",
       "SZKkRAxikiRJiRjEJEmSEjGISZIkJWIQkyRJSsQgJkmSlIhBTJIkKRGDmCRJUiIGMUmSpEQMYpIk\n",
       "SYkYxCRJkhIxiEmSJCViEJMkSUrEICZJkpSIQUySJCkRg5gkSVIiBjFJkqREDGKSJEmJGMQkSZIS\n",
       "MYhJkiQlUpC6AElphd6hD2UUN6lTBZVxdpzVSiVJUqdhEJM6uzKKGcbiJvUZQ2krVSNJnYq3JiVJ\n",
       "khIxiEmSJCViEJMkSUrEOWKSVvUw2/AqNxIpI49P2J/TGcT81GVJUkfkiJikVQUiAzibkexHIS/y\n",
       "LCekLkmSOipHxCSt6jDeqv05UkgeHyesRpI6NIOYpIbdxtdZxt4M5ZupS5GkjsogJqm+FcD73MAA\n",
       "jmAnKlKXI0kdlXPEJNU3gU0ILOSbvJO6FEnqyAxikurbkvlsyhWpy5Ckjs5bk5Lqe4fuzOVbwD9S\n",
       "l9KWrNO6nGvjup1Sp2YQk1TfvnzIvpyVuow2Z13W5Vwb1+2UOjVvTUqSJCViEJMkSUrEICZJkpSI\n",
       "QUySJCkRJ+tLnV0FlU2eMF5BZStVI0mdikFM6uR8dUILuJo7qWFTIkWsxx2cyxhG8Qal3MFS9iOw\n",
       "lL05hT1dt1PSqrw1KUnNNZTzGcGBnMSBLOA0JrI+UEJ3JjOSwRTxAs9zXOoyJbU9johJUnM9xen8\n",
       "hSEA1LApr7MVsIwzeQqAUqaxiK8mrFBSG+WImCQ1x13szjK+wqkcxEgGU8CrVFEEVNW2CVQTyU9X\n",
       "pKS2qllBLITQI4QwLoTwegjhiRDCeo20uzOEMCeEMG1d+ktSm7Wc7gQW0JvlPMQ2VDEg80lIWpek\n",
       "dqG5I2IXAeNjjNsBTwEXN9LubsgM269bf0lqmw7haaCAK3maci6mgMmZT2LKsiS1DyHGdf93RQih\n",
       "HPh6jHFOCGETYEKMsX8jbfsCj8cYd1zH/jHG6H9hSkom9A/9WmOtyVgeZ7ToMSUll21uae6IWK8Y\n",
       "4xyAGOMHQK8c95ckSWq31vrUZAjhSWDjurtYOeR+WQPNmzsUv8b+IYRRdTYnxBgnNPN8kiRJzRZC\n",
       "2AvYq6n91hrEYoz7r+Gkc0IIG9e5tfhhE8/fpP4xxlFNPL4kSVKrywwOTfh0O4RweTb9mntr8jHg\n",
       "5MzPJwGPrqFtoP5TRE3pL0mS1KE0d7L+BsADQB9gJnB0jHF+CGFT4PYY40GZdr9n5XDdhsAc4PIY\n",
       "492N9W/kXE7Wl5RU6B36UEZxix60gkqXmZI6nmxzS7OCWC4ZxCRJUnuRq6cmJUmStI4MYpIkSYkY\n",
       "xCRJkhIxiEmSJCViEJMkSUrEICZJkpSIQUySJCkRg5gkSVIiBjFJkqREDGKSJEmJGMQkSZISMYhJ\n",
       "kiQlYhCTJElKxCAmSZKUiEFMkiQpEYOYJElSIgYxSZKkRAxikiRJiRjEJEmSEjGISZIkJWIQkyRJ\n",
       "SsQgJkmSlIhBTJIkKRGDmCRJUiIGMUmSpEQMYpIkSYkYxCRJkhIxiEmSJCViEJMkSUrEICZJkpSI\n",
       "QUySJCkRg5gkSVIiBjFJkqREDGKSJEmJGMQkSZISMYhJkiQlYhCTJElKxCAmSZKUiEFMkiQpEYOY\n",
       "JElSIgYxSZKkRAxikiRJiRjEJEmSEjGISZIkJWIQkyRJSsQgJkmSlIhBTJIkKRGDmCRJUiIGMUmS\n",
       "pEQMYpIkSYkYxCRJkhIxiEmSJCViEJMkSUrEICZJkpSIQUySJCkRg5gkSVIiBjFJkqREDGKSJEmJ\n",
       "GMQkSZISMYhJkiQlYhCTJElKxCAmSZKUiEFMkiQpEYOYJElSIgYxSZKkRAxikiRJiRjEJEmSEmlW\n",
       "EAsh9AghjAshvB5CeCKEsF4j7e4MIcwJIUxbbf/lIYTZIYQpmT9Dm1OPJElSe9LcEbGLgPExxu2A\n",
       "p4CLG2l3NzCkkc9uijEOyPz5WzPrkSRJajcKmtn/EODrmZ9/C0xgZThbRYzxnyGEvo0cIzSzBknt\n",
       "VOgd+lBGcdYdKqiMs+OsVixJknKquUGsV4xxDkCM8YMQQq91OMbZIYQTgMnABTHGBc2sSVJ7UUYx\n",
       "w1icdfsx/9/e/YfaXddxHH++TINycyjDO3OushIJMqn8AQqd1NWc4sI/QhbOraIgLFGwUoxGGDWD\n",
       "lREGk6QMQiKFrah0oCPCH6za3MxtTgtrM6+ZFSwaLXn1x/lOj3f33vO9O+d+P9/jfT3gwvl+z+d7\n",
       "vy/efM857++P8z0cP4tpIiIa17cRk7QZGOudBRi4ZZLhnuH67wC+atuSbgXWA5+cJsvanskttrfM\n",
       "cH0RERERQyepA3RmulzfRsz20mlWOi5pzPa4pEXACzNZue2/9UzeCfysz/i1M/n/EREREU2oDg5t\n",
       "OTwt6St1lhv0Yv1NwOrq8TXAxmnGignXg1XN22FXAk8MmCciIiJiZAzaiK0DlkraA1wMfANA0imS\n",
       "fn54kKQfAw8DZ0j6s6Q11VO3SdohaTvdi/6vHzBPRLwefJMbWM+nS8eIiJhtA12sb/sl4JJJ5v8V\n",
       "uLxneuUUy68aZP0RERERoyx31o+IiIgoJI3YLKi+ORFHKfUbTOo3mNRvMKnfYFK/wYxi/dKIzY5O\n",
       "6QAjrlM6wIjrlA4w4jqlA4y4TukAI65TOsCI65QOMFOD3tA1ImL4bmR96QgREU3IEbGIiIiIQmTP\n",
       "9Gb4ZUgajaARUd9xMOkvTf6X7l0Hj5sw/yBwaLZDRUQMh+2+v6c9Mo1YRLz+6Ey9a6a/Nend3juL\n",
       "kSIiGpVTkxERERGFpBGLiIiIKCSNWEREREQhacSGQNKJkh6QtEfS/ZIWTDP2GEm/l7SpyYxtVqd+\n",
       "khZLelDSHyTtlPT5ElnbQtIySbslPSXpi1OM+Y6kvZK2Szq76Yxt1q9+klZKerz6+42k95TI2VZ1\n",
       "tr9q3DmSDkm6ssl8bVfz9duRtE3SE5Ieajpjm9V4/Z4gaVP13rdT0uoCMWvLxfpDIGkd8Hfbt1Ub\n",
       "xYm2vzTF2OuB9wMn2L6iyZxtVad+khYBi2xvlzQP+B2wwvbuApGLknQM8BRwMfAcsBW4qrcWki4F\n",
       "rrV9maTzgNttn18k8DS0WKcxb9LvTU7uAAe9z38ZaJ316nc+sMv2vyQtA9a2sX4l1Klfz7jNwH+A\n",
       "u2zf13TWNqq5/S0AHgY+bHu/pIW2XywSuGVq1u8mup+xN0laCOwBxmz/r0TmfnJD1+FYAXywevxD\n",
       "YAtwRCMmaTGwHPgacENT4UZA3/rZfh54vnp8QNIu4FRgzjViwLnAXtvPAki6h24Ne2uxArgbwPZj\n",
       "khZIGrM93njaaQzaVB2lvvWz/WjP+EfpbmvRVWf7A/gc8FPgnGbjtV6d+q0E7rW9HyBN2GvUqZ+B\n",
       "+dXj+XR39FvZhEFOTQ7LyYc/4KqG4eQpxn0LuJHuRhKvqls/ACS9DTgbeGzWk7XTqUBvA7OPIxuF\n",
       "iWP2TzJmrqpTv16fAn45q4lGS9/6SXoL8FHb36N7R7h4VZ3t7wzgJEkPSdoq6erG0rVfnfp9F3i3\n",
       "pOeAx4HrGsp2VHJErCZJm4Gx3ll0G6pbJhl+RKMl6TJgvDq11mGOvTkNWr+e/zOP7l72dbYPDDVk\n",
       "xASSPgSsAS4snWXEfBvovXZnTr3fDcGxwPuAi4DjgUckPWL76bKxRsZHgG22L5L0DmCzpLPa+pmR\n",
       "Rqwm20unek7S+OHTPtW1TC9MMuwC4ApJy4E3AfMl3W171SxFbpUh1A9Jx9Jtwn5ke+MsRR0F+4El\n",
       "PdOLq3kTx5zWZ8xcVad+SDoL2AAss/2PhrKNgjr1+wBwjyQBC4FLJR2ynS8p1avfPuBF2weBg5J+\n",
       "DbwXSCNWr35rgK8D2H5G0p+AM4HfNpJwhnJqcjg2Aaurx9cARzQJtm+2vcT26cBVwINzpQmroW/9\n",
       "KncBT9q+vYlQLbYVeKekt0p6I93taeIH3CZgFbxy4fk/23Z9WEF96ydpCXAvcLXtZwpkbLO+9bN9\n",
       "evX3dro7T59NE/aKOq/fjcCFkt4g6c3AecCuhnO2VZ36PQtcAiBpjO6p3j82mnIGckRsONYBP5H0\n",
       "CbobwMcAJJ0C3Gn78pLhRkDf+km6APg4sFPSNrqnL2+2/atSoUux/bKka4EH6O5Mfd/2Lkmf6T7t\n",
       "DbZ/IWm5pKeBf9PdQwzq1Q/4MnAScEd1VOeQ7XPLpW6PmvV7zSKNh2yxmq/f3ZLuB3YALwMbbD9Z\n",
       "MHZr1Nz+bgV+IGlHtdgXbL9UKHJfuX1FRERERCE5NRkRERFRSBqxiIiIiELSiEVEREQUkkYsIiIi\n",
       "opA0YhERERGFpBGLiIiIKCSNWEREREQh/wc6Qowb0PiXWgAAAABJRU5ErkJggg==\n"
      ],
      "text/plain": [
       "<matplotlib.figure.Figure at 0xa90e320>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the word vectors you trained\n",
    "\n",
    "_, wordVectors0, _ = load_saved_params()\n",
    "wordVectors = (wordVectors0[:nWords,:] + wordVectors0[nWords:,:])\n",
    "visualizeWords = [\"the\", \"a\", \"an\", \",\", \".\", \"?\", \"!\", \"``\", \"''\", \"--\", \"good\", \"great\", \"cool\", \"brilliant\", \"wonderful\", \"well\", \"amazing\", \"worth\", \"sweet\", \"enjoyable\", \"boring\", \"bad\", \"waste\", \"dumb\", \"annoying\"]\n",
    "visualizeIdx = [tokens[word] for word in visualizeWords]\n",
    "visualizeVecs = wordVectors[visualizeIdx, :]\n",
    "temp = (visualizeVecs - np.mean(visualizeVecs, axis=0))\n",
    "covariance = 1.0 / len(visualizeIdx) * temp.T.dot(temp)\n",
    "U,S,V = np.linalg.svd(covariance)\n",
    "coord = temp.dot(U[:,0:2]) \n",
    "\n",
    "for i in xrange(len(visualizeWords)):\n",
    "    plt.text(coord[i,0], coord[i,1], visualizeWords[i], bbox=dict(facecolor='green', alpha=0.1))\n",
    "    \n",
    "plt.xlim((np.min(coord[:,0]), np.max(coord[:,0])))\n",
    "plt.ylim((np.min(coord[:,1]), np.max(coord[:,1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Sentiment Analysis\n",
    "\n",
    "Now, with the word vectors you trained, we are going to perform a simple sentiment analysis.\n",
    "\n",
    "For each sentence in the Stanford Sentiment Treebank dataset, we are going to use the average of all the word vectors in that sentence as its feature, and try to predict the sentiment level of the said sentence. The sentiment level of the phrases are represented as real values in the original dataset, here we'll just use five classes:\n",
    "\n",
    "    \"very negative\", \"negative\", \"neutral\", \"positive\", \"very positive\"\n",
    "    \n",
    "which are represented by 0 to 4 in the code, respectively.\n",
    "\n",
    "For this part, you will learn to train a softmax regressor with SGD, and perform train/dev validation to improve generalization of your regressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now, implement some helper functions\n",
    "\n",
    "def getSentenceFeature(tokens, wordVectors, sentence):\n",
    "    \"\"\" Obtain the sentence feature for sentiment analysis by averaging its word vectors \"\"\"\n",
    "    ###################################################################\n",
    "    # Implement computation for the sentence features given a         #\n",
    "    # sentence.                                                       #\n",
    "    # Inputs:                                                         #\n",
    "    #   - tokens: a dictionary that maps words to their indices in    #\n",
    "    #             the word vector list                                #\n",
    "    #   - wordVectors: word vectors for all tokens                    #\n",
    "    #   - sentence: a list of words in the sentence of interest       #\n",
    "    # Output:                                                         #\n",
    "    #   - sentVector: feature vector for the sentence                 #\n",
    "    ###################################################################\n",
    "    \n",
    "    sentVector = np.zeros((wordVectors.shape[1],))\n",
    "    \n",
    "    ### YOUR CODE HERE\n",
    "    for w in sentence:\n",
    "        sentVector += wordVectors[tokens[w]]\n",
    "    sentVector /= len(sentVector) \n",
    "    ### END YOUR CODE\n",
    "    \n",
    "    return sentVector\n",
    "\n",
    "def softmaxRegression(features, labels, weights, regularization = 0.0, nopredictions = False):\n",
    "    \"\"\" Softmax Regression \"\"\"\n",
    "    ###################################################################\n",
    "    # Implement softmax regression with weight regularization.        #\n",
    "    # Inputs:                                                         #\n",
    "    #   - features: feature vectors, each row is a feature vector     #\n",
    "    #   - labels: labels corresponding to the feature vectors         #\n",
    "    #   - weights: weights of the regressor                           #\n",
    "    #   - regularization: L2 regularization constant                  #\n",
    "    # Output:                                                         #\n",
    "    #   - cost: cost of the regressor                                 #\n",
    "    #   - grad: gradient of the regressor cost with respect to its    #\n",
    "    #           weights                                               #\n",
    "    #   - pred: label predictions of the regressor (you might find    #\n",
    "    #           np.argmax helpful)                                    #\n",
    "    ###################################################################\n",
    "    \n",
    "    prob = softmax(features.dot(weights))\n",
    "    if len(features.shape) > 1:\n",
    "        N = features.shape[0]\n",
    "    else:\n",
    "        N = 1\n",
    "    # A vectorized implementation of    1/N * sum(cross_entropy(x_i, y_i)) + 1/2*|w|^2\n",
    "    cost = np.sum(-np.log(prob[range(N), labels])) / N \n",
    "    cost += 0.5 * regularization * np.sum(weights ** 2)\n",
    "    \n",
    "    ### YOUR CODE HERE: compute the gradients and predictions\n",
    "    delta = (prob - np.eye(prob.shape[1])[labels]) / N\n",
    "    grad = features.T.dot(delta)\n",
    "    grad += regularization * weights\n",
    "    pred = np.argmax(prob, axis=1)\n",
    "    \n",
    "    \n",
    "    ### END YOUR CODE\n",
    "    \n",
    "    if nopredictions:\n",
    "        return cost, grad\n",
    "    else:\n",
    "        return cost, grad, pred\n",
    "\n",
    "def precision(y, yhat):\n",
    "    \"\"\" Precision for classifier \"\"\"\n",
    "    assert(y.shape == yhat.shape)\n",
    "    return np.sum(y == yhat) * 100.0 / y.size\n",
    "\n",
    "def softmax_wrapper(features, labels, weights, regularization = 0.0):\n",
    "    cost, grad, _ = softmaxRegression(features, labels, weights, regularization)\n",
    "    return cost, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Gradient check for softmax regression ====\n",
      "Gradient check passed!\n",
      "\n",
      "=== For autograder ===\n",
      "(1.9499665595796194, array([[ 0.07188086, -0.04024054,  0.11385216,  0.21435741,  0.07139013],\n",
      "       [-0.13099573, -0.09200812,  0.02695393,  0.11196531,  0.02201353],\n",
      "       [ 0.02748989, -0.06847299,  0.00408506, -0.0043378 , -0.07766546],\n",
      "       [ 0.19617063,  0.29955208, -0.09331083, -0.14361121, -0.02769059],\n",
      "       [ 0.30217038,  0.19037185, -0.17510531, -0.14508693, -0.2393616 ],\n",
      "       [-0.04843503, -0.0774563 ,  0.33194379,  0.17868769,  0.0928552 ],\n",
      "       [-0.15329447, -0.16801972, -0.23104123, -0.13526044,  0.08407597],\n",
      "       [ 0.17971673,  0.12378277, -0.09350615,  0.02924814, -0.11033724],\n",
      "       [-0.34770358,  0.09551068,  0.14846368,  0.19625247,  0.0155008 ],\n",
      "       [-0.07381084, -0.01718123,  0.12600253, -0.04085898,  0.1018485 ]]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 1], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "# Gradient check always comes first\n",
    "random.seed(314159)\n",
    "np.random.seed(265)\n",
    "dummy_weights = 0.1 * np.random.randn(dimVectors, 5)\n",
    "dummy_features = np.zeros((10, dimVectors))\n",
    "dummy_labels = np.zeros((10,), dtype=np.int32)    \n",
    "for i in xrange(10):\n",
    "    words, dummy_labels[i] = dataset.getRandomTrainSentence()\n",
    "    dummy_features[i, :] = getSentenceFeature(tokens, wordVectors, words)\n",
    "print \"==== Gradient check for softmax regression ====\"\n",
    "gradcheck_naive(lambda weights: softmaxRegression(dummy_features, dummy_labels, weights, 1.0, nopredictions = True), dummy_weights)\n",
    "\n",
    "print \"\\n=== For autograder ===\"\n",
    "print softmaxRegression(dummy_features, dummy_labels, dummy_weights, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 cost: 1.98273085338 x= [[-0.11613604  1.23910171  1.81937381  0.17837145  0.72068861]\n",
      " [-0.52998143 -0.46378724  0.16819183  1.32352212 -1.50260915]\n",
      " [ 0.99863661 -0.08596132 -1.00799832  1.57374952  1.07997454]\n",
      " [ 0.90962537  1.53056871  0.04646067 -0.10776926  0.97616912]\n",
      " [ 0.27372574 -0.49929365 -0.23138742  1.3190474   0.27042662]\n",
      " [ 1.30623108  1.21845861 -0.2228999   0.21823136 -0.51687483]\n",
      " [ 1.48808397 -0.26382931  0.79372256 -1.11939791  0.82267692]\n",
      " [ 0.48300857  0.51612528  0.62281949 -0.34027982 -1.77291306]\n",
      " [ 0.9009095   0.74636429 -0.99107812  0.18093275 -0.12220783]\n",
      " [ 0.20496845 -0.30156884 -0.31839755 -0.56801767  0.30093173]]\n",
      "200 cost: 1.91271092347 x= [[-0.09456317  1.0311869   1.6971316   0.21313914  0.6637381 ]\n",
      " [-0.53981101 -0.46884903  0.1348497   1.22636035 -1.27070642]\n",
      " [ 0.90085448 -0.15873236 -0.9792487   1.54832176  1.02691256]\n",
      " [ 0.77344149  1.60249906  0.01955033 -0.13094029  0.8016142 ]\n",
      " [ 0.19493863 -0.26304171 -0.27227765  1.22239644  0.1529865 ]\n",
      " [ 1.21887024  0.99571406 -0.14496205  0.13551383 -0.37447238]\n",
      " [ 1.42028373 -0.16969313  0.77200014 -1.09663824  0.64709351]\n",
      " [ 0.44796422  0.54382105  0.51631988 -0.29524039 -1.66180571]\n",
      " [ 0.89413866  0.67189238 -0.75755104  0.01743243 -0.17255069]\n",
      " [ 0.2522422  -0.29298543 -0.27243576 -0.58728175  0.27710828]]\n",
      "300 cost: 1.81430416736 x= [[-0.09983006  0.99038518  1.52505179  0.2189585   0.57378115]\n",
      " [-0.556919   -0.42042064  0.08031302  1.1328021  -1.07487325]\n",
      " [ 0.81182664 -0.22734443 -0.94930211  1.52236608  0.97923681]\n",
      " [ 0.68094129  1.47847449  0.06866875 -0.11775673  0.69182226]\n",
      " [ 0.17846483 -0.38462829 -0.1727082   1.19313201  0.1316221 ]\n",
      " [ 1.11778691  0.93004433 -0.13405328  0.04070288 -0.281448  ]\n",
      " [ 1.35642661 -0.07132876  0.7464704  -1.0800411   0.48607039]\n",
      " [ 0.44420642  0.39482929  0.49619417 -0.22836059 -1.51715379]\n",
      " [ 0.85175306  0.83133073 -0.64746657 -0.16324921 -0.27526454]\n",
      " [ 0.2879613  -0.24049627 -0.2491789  -0.61276988  0.24480559]]\n",
      "400 cost: 1.780228046 x= [[-0.11731819  0.97851781  1.35620594  0.2338584   0.4808252 ]\n",
      " [-0.57634965 -0.36751148  0.02441803  1.05454885 -0.9019523 ]\n",
      " [ 0.73059512 -0.28964803 -0.9197281   1.49566728  0.9359072 ]\n",
      " [ 0.61276469  1.3322373   0.12602887 -0.11687558  0.60671327]\n",
      " [ 0.19120366 -0.55285103 -0.05518135  1.14466068  0.13660449]\n",
      " [ 1.0152172   0.89399499 -0.1388618  -0.03296173 -0.20841373]\n",
      " [ 1.29732879  0.01882699  0.72219676 -1.06522471  0.34068415]\n",
      " [ 0.45461514  0.23079622  0.49628488 -0.18280374 -1.3738491 ]\n",
      " [ 0.79551883  1.01326971 -0.57320273 -0.3058409  -0.38405553]\n",
      " [ 0.31683458 -0.18556299 -0.2318101  -0.63297072  0.21288369]]\n",
      "500 cost: 1.88282990429 x= [[-0.134026    0.88981765  1.23377212  0.28688132  0.40317404]\n",
      " [-0.59369202 -0.34788572 -0.01667889  1.00069688 -0.74325684]\n",
      " [ 0.65624625 -0.34400486 -0.89182067  1.46810347  0.89612233]\n",
      " [ 0.55169278  1.29572415  0.13656378 -0.16484435  0.52122641]\n",
      " [ 0.20369092 -0.53045952 -0.01944161  1.01138545  0.12482816]\n",
      " [ 0.92209092  0.79055069 -0.11767602 -0.06078237 -0.13686198]\n",
      " [ 1.24371287  0.09166392  0.70329443 -1.04907175  0.21108562]\n",
      " [ 0.46394484  0.17540377  0.46258316 -0.19160937 -1.25299303]\n",
      " [ 0.7448725   1.05633006 -0.46323283 -0.36863358 -0.47063382]\n",
      " [ 0.34307215 -0.15948484 -0.20663994 -0.63945026  0.18670626]]\n",
      "600 cost: 1.97791845161 x= [[-0.14535394  0.7827744   1.12313033  0.35066741  0.33767004]\n",
      " [-0.60754091 -0.33968989 -0.0550662   0.95987568 -0.59805085]\n",
      " [ 0.58812107 -0.39240315 -0.86505597  1.44067225  0.85964382]\n",
      " [ 0.4910756   1.29467425  0.14309342 -0.22744888  0.43744947]\n",
      " [ 0.20538016 -0.45080342  0.01073429  0.85486192  0.10180654]\n",
      " [ 0.84183789  0.67264383 -0.10020528 -0.06919771 -0.06807502]\n",
      " [ 1.19552185  0.15388002  0.68624756 -1.03381072  0.09546037]\n",
      " [ 0.46688961  0.1573673   0.43381373 -0.22092649 -1.15030886]\n",
      " [ 0.70638147  1.05303402 -0.3670247  -0.3967269  -0.53990276]\n",
      " [ 0.36834155 -0.14416729 -0.18356616 -0.64059772  0.16516186]]\n",
      "700 cost: 1.93624024205 x= [[-0.12785677  0.61215259  1.06703336  0.37519154  0.31150392]\n",
      " [-0.60960239 -0.35876183 -0.07490954  0.91271773 -0.45476773]\n",
      " [ 0.52520215 -0.43437905 -0.84048804  1.41463221  0.825574  ]\n",
      " [ 0.4007658   1.38515387  0.09106951 -0.241909    0.31959671]\n",
      " [ 0.14329863 -0.21294467 -0.06307788  0.78777696  0.00475979]\n",
      " [ 0.79501225  0.49852699 -0.04569823 -0.10489668  0.02410188]\n",
      " [ 1.15456447  0.20193417  0.67513059 -1.02394259 -0.00487143]\n",
      " [ 0.43572775  0.22879858  0.35710497 -0.21022648 -1.09760421]\n",
      " [ 0.71571748  0.93515914 -0.21487556 -0.46961411 -0.54986951]\n",
      " [ 0.39991257 -0.1528446  -0.14916301 -0.65157999  0.15628848]]\n",
      "800 cost: 1.84974735006 x= [[-0.12921984  0.58387315  0.96839883  0.36468953  0.25757595]\n",
      " [-0.61724907 -0.33065255 -0.11094812  0.86000594 -0.33608016]\n",
      " [ 0.4680272  -0.47486374 -0.81621912  1.39014741  0.7951052 ]\n",
      " [ 0.34036497  1.30575027  0.10245403 -0.21309933  0.25089782]\n",
      " [ 0.12524339 -0.28446625 -0.02285241  0.80092044 -0.01584608]\n",
      " [ 0.73720201  0.45729753 -0.04175832 -0.16556096  0.07937645]\n",
      " [ 1.11575476  0.25709134  0.66034178 -1.01899874 -0.0977222 ]\n",
      " [ 0.42708885  0.1442633   0.34578905 -0.16377448 -1.01492267]\n",
      " [ 0.69871094  1.02311753 -0.15385392 -0.582941   -0.6043807 ]\n",
      " [ 0.42353321 -0.12231625 -0.13242872 -0.67154837  0.13959089]]\n",
      "900 cost: 1.75924413675 x= [[-0.13889463  0.60120973  0.85588744  0.35248098  0.19852029]\n",
      " [-0.62686504 -0.28959102 -0.15265822  0.81269962 -0.23244922]\n",
      " [ 0.41593802 -0.5123207  -0.79288092  1.36666296  0.76750445]\n",
      " [ 0.29592398  1.17845157  0.1402137  -0.1833996   0.20136141]\n",
      " [ 0.12723171 -0.44714805  0.06488147  0.81856994 -0.01245773]\n",
      " [ 0.67748818  0.45953874 -0.06064443 -0.22135828  0.11969572]\n",
      " [ 1.07975935  0.31044399  0.64463837 -1.01535703 -0.1819309 ]\n",
      " [ 0.42856163  0.01699662  0.36425627 -0.12014461 -0.92870435]\n",
      " [ 0.67127962  1.16865799 -0.13676829 -0.68689684 -0.66839606]\n",
      " [ 0.44261399 -0.08178663 -0.1241815  -0.69039523  0.12185113]]\n",
      "1000 cost: 1.85713799198 x= [[ -1.58688449e-01   5.70228301e-01   7.74292997e-01   3.81491307e-01\n",
      "    1.40930271e-01]\n",
      " [ -6.39225104e-01  -2.69336869e-01  -1.83649559e-01   7.85664104e-01\n",
      "   -1.40222418e-01]\n",
      " [  3.68547244e-01  -5.45055238e-01  -7.71516260e-01   1.34338539e+00\n",
      "    7.42349185e-01]\n",
      " [  2.69291081e-01   1.12054989e+00   1.47323269e-01  -2.06722264e-01\n",
      "    1.61536886e-01]\n",
      " [  1.53337494e-01  -4.89942382e-01   9.74600956e-02   7.43810066e-01\n",
      "   -1.03891519e-03]\n",
      " [  6.13589174e-01   4.19272445e-01  -6.01863210e-02  -2.33986636e-01\n",
      "    1.52102187e-01]\n",
      " [  1.04606365e+00   3.53522386e-01   6.32376474e-01  -1.00873923e+00\n",
      "   -2.57787780e-01]\n",
      " [  4.42327110e-01  -4.25240542e-02   3.57853970e-01  -1.29639247e-01\n",
      "   -8.46469963e-01]\n",
      " [  6.30284318e-01   1.22772794e+00  -9.17616108e-02  -7.16686834e-01\n",
      "   -7.31641594e-01]\n",
      " [  4.56763306e-01  -5.92044251e-02  -1.10460199e-01  -6.95393875e-01\n",
      "    1.04975330e-01]]\n",
      "1100 cost: 1.95492917678 x= [[-0.17050342  0.49754028  0.70697367  0.43023002  0.09692317]\n",
      " [-0.64794742 -0.26703213 -0.20980259  0.77039168 -0.05390989]\n",
      " [ 0.32509393 -0.57363835 -0.75173563  1.32084409  0.71918276]\n",
      " [ 0.23707811  1.12338896  0.14411122 -0.25602626  0.11495871]\n",
      " [ 0.16249059 -0.42845876  0.11120383  0.62506746 -0.01004194]\n",
      " [ 0.5616644   0.34213046 -0.05461395 -0.22376971  0.18867734]\n",
      " [ 1.01613743  0.38750857  0.6218664  -1.00102746 -0.32495792]\n",
      " [ 0.44698285 -0.04330292  0.34507829 -0.16612008 -0.78228031]\n",
      " [ 0.60344971  1.21175597 -0.04244555 -0.70688547 -0.77532739]\n",
      " [ 0.47165421 -0.05236738 -0.09594904 -0.69311833  0.09257828]]\n",
      "1200 cost: 1.95198069625 x= [[-0.15822238  0.35628728  0.68492617  0.45747048  0.08627685]\n",
      " [-0.6472982  -0.29197596 -0.21981824  0.7512713   0.03467781]\n",
      " [ 0.28489489 -0.59780688 -0.73414645  1.29989397  0.69738299]\n",
      " [ 0.17858304  1.22045827  0.09085857 -0.2795478   0.03575241]\n",
      " [ 0.118234   -0.2025887   0.03481083  0.55520721 -0.08503335]\n",
      " [ 0.53595809  0.20309628 -0.01462936 -0.22873389  0.24829963]\n",
      " [ 0.99125986  0.41022408  0.61609341 -0.99601102 -0.38227267]\n",
      " [ 0.42342954  0.04552542  0.28803709 -0.18008897 -0.7593549 ]\n",
      " [ 0.6154083   1.08015308  0.06102727 -0.72222492 -0.76883429]\n",
      " [ 0.49228456 -0.06924157 -0.07093153 -0.69600642  0.09056145]]\n",
      "1300 cost: 1.86454563938 x= [[-0.1539544   0.32847144  0.63295136  0.43824053  0.05817894]\n",
      " [-0.64893065 -0.2785862  -0.24068762  0.71901929  0.10817171]\n",
      " [ 0.24831832 -0.62171604 -0.71718923  1.28110862  0.67787747]\n",
      " [ 0.13402913  1.18066566  0.08215171 -0.2452642  -0.01277468]\n",
      " [ 0.09318878 -0.22545039  0.03745707  0.59107433 -0.1118585 ]\n",
      " [ 0.50656834  0.16953216 -0.00851488 -0.27271588  0.28505905]\n",
      " [ 0.96802749  0.44020866  0.60765131 -0.99619572 -0.435445  ]\n",
      " [ 0.40974622  0.00754894  0.27487277 -0.14165627 -0.7172533 ]\n",
      " [ 0.61627398  1.11558085  0.10405148 -0.80247133 -0.79076919]\n",
      " [ 0.50903146 -0.05403246 -0.05796415 -0.71197253  0.08341767]]\n",
      "1400 cost: 1.76404230278 x= [[-0.16078401  0.36534914  0.55441998  0.41441683  0.01821356]\n",
      " [-0.65407014 -0.24395472 -0.27119571  0.68817448  0.16939586]\n",
      " [ 0.21514901 -0.64467688 -0.70083595  1.26358591  0.6604028 ]\n",
      " [ 0.10688998  1.06561804  0.11302413 -0.20620348 -0.03857906]\n",
      " [ 0.09420991 -0.38702507  0.11037926  0.63772129 -0.10397415]\n",
      " [ 0.4703559   0.19618446 -0.03234899 -0.3176226   0.30481418]\n",
      " [ 0.94592012  0.47298796  0.5968478  -0.99727906 -0.48453716]\n",
      " [ 0.40963283 -0.10040284  0.30076776 -0.10001798 -0.66236398]\n",
      " [ 0.60108606  1.24348568  0.09333691 -0.8836894  -0.83244839]\n",
      " [ 0.52110969 -0.02150713 -0.05570225 -0.72830339  0.07281831]]\n",
      "1500 cost: 1.84323141388 x= [[ -1.80756905e-01   3.71837027e-01   4.95262216e-01   4.28688303e-01\n",
      "   -2.60202075e-02]\n",
      " [ -6.63510418e-01  -2.22325494e-01  -2.94686697e-01   6.73841284e-01\n",
      "    2.21866011e-01]\n",
      " [  1.85149321e-01  -6.64825731e-01  -6.86062688e-01   1.24642590e+00\n",
      "    6.44602351e-01]\n",
      " [  9.94904362e-02   9.94485359e-01   1.25112762e-01  -2.16024086e-01\n",
      "   -5.19294896e-02]\n",
      " [  1.25905831e-01  -4.73145689e-01   1.48801728e-01   5.98738355e-01\n",
      "   -7.92389407e-02]\n",
      " [  4.25073314e-01   1.95983864e-01  -4.34046005e-02  -3.24911677e-01\n",
      "    3.15137347e-01]\n",
      " [  9.24531789e-01   4.99651996e-01   5.88182836e-01  -9.95069994e-01\n",
      "   -5.29332285e-01]\n",
      " [  4.25506546e-01  -1.66084985e-01   3.10394189e-01  -1.05525755e-01\n",
      "   -6.03553037e-01]\n",
      " [  5.66513597e-01   1.31748100e+00   1.01377835e-01  -9.00697490e-01\n",
      "   -8.81999854e-01]\n",
      " [  5.27999325e-01  -1.75869720e-04  -4.99689905e-02  -7.32254771e-01\n",
      "    6.10342070e-02]]\n",
      "1600 cost: 1.94362512685 x= [[ -1.91954239e-01   3.17280657e-01   4.55195953e-01   4.70429480e-01\n",
      "   -5.57115865e-02]\n",
      " [ -6.69428790e-01  -2.24503281e-01  -3.11226365e-01   6.71836352e-01\n",
      "    2.73031034e-01]\n",
      " [  1.57597149e-01  -6.81735639e-01  -6.72780824e-01   1.22982288e+00\n",
      "    6.29933953e-01]\n",
      " [  8.38104734e-02   1.00565157e+00   1.18322262e-01  -2.61259119e-01\n",
      "   -7.72884877e-02]\n",
      " [  1.38578184e-01  -4.14947415e-01   1.52547774e-01   4.98112283e-01\n",
      "   -8.08747980e-02]\n",
      " [  3.90593894e-01   1.40610484e-01  -4.14225774e-02  -3.04674158e-01\n",
      "    3.33872965e-01]\n",
      " [  9.05832459e-01   5.17500060e-01   5.81597182e-01  -9.90524895e-01\n",
      "   -5.68457048e-01]\n",
      " [  4.31388282e-01  -1.53805012e-01   3.03396428e-01  -1.45295489e-01\n",
      "   -5.62955889e-01]\n",
      " [  5.46350844e-01   1.29082763e+00   1.28886792e-01  -8.70605045e-01\n",
      "   -9.10236650e-01]\n",
      " [  5.36475858e-01   8.01419641e-04  -4.06371161e-02  -7.27118080e-01\n",
      "    5.37617717e-02]]\n",
      "1700 cost: 1.95725346819 x= [[-0.18143785  0.19546451  0.45383808  0.49750894 -0.05582941]\n",
      " [-0.66719267 -0.25238637 -0.3137363   0.66648769  0.32894919]\n",
      " [ 0.13200314 -0.69548313 -0.66129601  1.21457813  0.61596119]\n",
      " [ 0.04329666  1.10692868  0.06738949 -0.28903935 -0.13418515]\n",
      " [ 0.10295269 -0.19858009  0.07644269  0.43073833 -0.14340244]\n",
      " [ 0.37845733  0.0239351  -0.00791148 -0.29541767  0.37523007]\n",
      " [ 0.89089929  0.52613401  0.57898112 -0.98757942 -0.60088595]\n",
      " [ 0.41192405 -0.05687529  0.25592413 -0.16901601 -0.55826971]\n",
      " [ 0.56044505  1.15445616  0.2072092  -0.8589553  -0.89388038]\n",
      " [ 0.55052846 -0.02044985 -0.02154962 -0.72553633  0.05550749]]\n",
      "1800 cost: 1.86845028021 x= [[ -1.73512083e-01   1.70053605e-01   4.28799789e-01   4.73919410e-01\n",
      "   -6.80335479e-02]\n",
      " [ -6.65591557e-01  -2.47124000e-01  -3.24637010e-01   6.44758965e-01\n",
      "    3.75197862e-01]\n",
      " [  1.08677502e-01  -7.09685728e-01  -6.50298327e-01   1.20143946e+00\n",
      "    6.03470655e-01]\n",
      " [  8.47151094e-03   1.09055160e+00   5.09082742e-02  -2.53440622e-01\n",
      "   -1.70502088e-01]\n",
      " [  7.40167300e-02  -1.95391802e-01   6.10848965e-02   4.78407384e-01\n",
      "   -1.73055403e-01]\n",
      " [  3.66075686e-01  -3.43685203e-03   2.29018947e-04  -3.30798508e-01\n",
      "    4.01384565e-01]\n",
      " [  8.77191384e-01   5.42067691e-01   5.74205728e-01  -9.89867086e-01\n",
      "   -6.31141028e-01]\n",
      " [  3.95907473e-01  -6.95947854e-02   2.41335885e-01  -1.33930809e-01\n",
      "   -5.40015390e-01]\n",
      " [  5.71090255e-01   1.16221297e+00   2.40832972e-01  -9.22076816e-01\n",
      "   -8.97360183e-01]\n",
      " [  5.62849785e-01  -1.38134749e-02  -1.14913024e-02  -7.38710274e-01\n",
      "    5.35714955e-02]]\n",
      "1900 cost: 1.76734721778 x= [[-0.17948131  0.22216645  0.36938992  0.44488334 -0.09730475]\n",
      " [-0.66877361 -0.21516307 -0.34784467  0.62264579  0.41045887]\n",
      " [ 0.08768244 -0.72417808 -0.63944558  1.18938632  0.59248996]\n",
      " [-0.00667413  0.98051305  0.08183055 -0.21165464 -0.18053802]\n",
      " [ 0.07723697 -0.36273544  0.12990024  0.53867174 -0.15911296]\n",
      " [ 0.3429199   0.04075572 -0.02630974 -0.36946287  0.40822798]\n",
      " [ 0.86347273  0.56360479  0.56621195 -0.99283552 -0.66006796]\n",
      " [ 0.39676281 -0.17022556  0.27189453 -0.09342946 -0.5021471 ]\n",
      " [ 0.56071146  1.28541988  0.21401777 -0.9904239  -0.92834652]\n",
      " [ 0.57011705  0.01494809 -0.01349229 -0.75290206  0.04644413]]\n",
      "2000 cost: 1.83981585556 x= [[-0.19869331  0.24605697  0.32580953  0.45316493 -0.13209509]\n",
      " [-0.67652683 -0.19463235 -0.36521545  0.61543914  0.43936602]\n",
      " [ 0.06881815 -0.73678655 -0.62978822  1.17749536  0.58263234]\n",
      " [-0.00311869  0.90971709  0.09650921 -0.21748453 -0.1792755 ]\n",
      " [ 0.11111774 -0.46200473  0.16875744  0.51493229 -0.12812649]\n",
      " [ 0.30936868  0.05964591 -0.04114047 -0.3725755   0.40672318]\n",
      " [ 0.84968253  0.58060491  0.55992982 -0.99231705 -0.68682345]\n",
      " [ 0.41369281 -0.23364684  0.28756749 -0.09810018 -0.45829333]\n",
      " [ 0.53024235  1.36088313  0.20526301 -0.99833687 -0.96884646]\n",
      " [ 0.57259348  0.03396634 -0.01216517 -0.7552919   0.03762656]]\n",
      "2100 cost: 1.94170981792 x= [[-0.20909628  0.20278851  0.30175913  0.49168018 -0.15266689]\n",
      " [-0.68094853 -0.19942939 -0.37540728  0.62042295  0.46942698]\n",
      " [ 0.0514435  -0.74680415 -0.62132728  1.16586638  0.57337978]\n",
      " [-0.00893978  0.92708701  0.08997736 -0.26190501 -0.19208208]\n",
      " [ 0.12567019 -0.40543363  0.16878491  0.42321474 -0.1251838 ]\n",
      " [ 0.28554367  0.01740841 -0.04060618 -0.3468405   0.4153442 ]\n",
      " [ 0.83787938  0.58941834  0.55570116 -0.98898013 -0.70972751]\n",
      " [ 0.42054451 -0.21407065  0.28357503 -0.13942142 -0.43176304]\n",
      " [ 0.51376439  1.32875695  0.22092195 -0.95713046 -0.988233  ]\n",
      " [ 0.57705939  0.031514   -0.00626314 -0.74808762  0.033121  ]]\n",
      "2200 cost: 1.95748061389 x= [[-0.19794033  0.09138369  0.31445866  0.51654315 -0.14461165]\n",
      " [-0.67744775 -0.22961448 -0.37224125  0.62142962  0.50622659]\n",
      " [ 0.03515506 -0.75434726 -0.61431443  1.15532153  0.56435859]\n",
      " [-0.04035918  1.03400294  0.04011602 -0.28986601 -0.23747075]\n",
      " [ 0.09205553 -0.19079516  0.09050317  0.3623705  -0.18318793]\n",
      " [ 0.28292418 -0.08726849 -0.00862832 -0.33219504  0.44752918]\n",
      " [ 0.8291873   0.59002805  0.55518737 -0.98696075 -0.72762986]\n",
      " [ 0.40217849 -0.11081911  0.23919322 -0.16512674 -0.43957518]\n",
      " [ 0.53089639  1.18782754  0.28814966 -0.93475823 -0.96420289]\n",
      " [ 0.58741335  0.00722369  0.00956641 -0.7444225   0.03726307]]\n",
      "2300 cost: 1.86805025442 x= [[-0.18891162  0.07480426  0.30267845  0.49021183 -0.14887645]\n",
      " [-0.67456772 -0.22665952 -0.37773516  0.60499033  0.53538251]\n",
      " [ 0.02032234 -0.76294015 -0.60747472  1.1466346   0.55637963]\n",
      " [-0.06754629  1.02371393  0.02418235 -0.25352009 -0.26401287]\n",
      " [ 0.06387535 -0.18766519  0.07213935  0.41767002 -0.20979288]\n",
      " [ 0.27958609 -0.10438459 -0.00122915 -0.36357702  0.46593109]\n",
      " [ 0.82118684  0.59875401  0.55238346 -0.99006665 -0.7448169 ]\n",
      " [ 0.38664674 -0.1171441   0.22663132 -0.13074595 -0.43315263]\n",
      " [ 0.54494515  1.19060493  0.31283894 -0.98968189 -0.96008656]\n",
      " [ 0.59650507  0.01093242  0.0168224  -0.7559352   0.03758443]]\n",
      "2400 cost: 1.76651574057 x= [[-0.19410647  0.13623388  0.25468798  0.45875239 -0.17128936]\n",
      " [-0.67672277 -0.19631812 -0.39620032  0.58751694  0.55506808]\n",
      " [ 0.00709209 -0.77240135 -0.60045262  1.13876715  0.54952781]\n",
      " [-0.075432    0.91772708  0.05655507 -0.21113658 -0.2647478 ]\n",
      " [ 0.06839777 -0.35790617  0.139703    0.48476743 -0.19218742]\n",
      " [ 0.2643646  -0.04975042 -0.02884022 -0.39874489  0.46550404]\n",
      " [ 0.81261896  0.61400385  0.54613403 -0.99369315 -0.76206797]\n",
      " [ 0.38830786 -0.21358655  0.25960374 -0.09076278 -0.40549197]\n",
      " [ 0.53715971  1.31141436  0.27726861 -1.05083722 -0.98487669]\n",
      " [ 0.60074421  0.03745037  0.01209477 -0.76861046  0.03233201]]\n",
      "2500 cost: 1.84172465525 x= [[-0.21247901  0.16630005  0.22235736  0.46591854 -0.19951773]\n",
      " [-0.68353997 -0.17808039 -0.4088999   0.58475214  0.57001777]\n",
      " [-0.00468295 -0.78041965 -0.59434828  1.13077979  0.54343218]\n",
      " [-0.06542101  0.8536281   0.07158953 -0.21777498 -0.2554757 ]\n",
      " [ 0.10324479 -0.45456396  0.17550043  0.46484002 -0.1585404 ]\n",
      " [ 0.23808382 -0.0235221  -0.04369295 -0.39786875  0.45778849]\n",
      " [ 0.80351086  0.62518735  0.54153657 -0.99359806 -0.77832561]\n",
      " [ 0.40589232 -0.27056006  0.27628735 -0.09701794 -0.37119886]\n",
      " [ 0.50913039  1.38126562  0.26219683 -1.0508577  -1.01936698]\n",
      " [ 0.60048888  0.05375977  0.01121839 -0.76931104  0.02525906]]\n",
      "2600 cost: 1.9439749583 x= [[-0.22201566  0.12952603  0.20794649  0.50313998 -0.21412629]\n",
      " [-0.68711487 -0.18462943 -0.41502379  0.59352162  0.5874629 ]\n",
      " [-0.01558445 -0.78631569 -0.58914221  1.12280672  0.53761606]\n",
      " [-0.06547855  0.87610244  0.06623184 -0.26256693 -0.26102672]\n",
      " [ 0.11852328 -0.39754227  0.17410936  0.37726432 -0.15310896]\n",
      " [ 0.22090322 -0.05834373 -0.04358083 -0.3689094   0.46084702]\n",
      " [ 0.79596165  0.62890874  0.53879449 -0.99061002 -0.79181949]\n",
      " [ 0.41329068 -0.24624702  0.27378427 -0.13929192 -0.35325984]\n",
      " [ 0.49491521  1.34550458  0.27145471 -1.00318447 -1.03341425]\n",
      " [ 0.60248395  0.04912839  0.01495054 -0.76066631  0.02228512]]\n",
      "2700 cost: 1.95514170748 x= [[-0.20911914  0.02272806  0.23062716  0.52503339 -0.1996262 ]\n",
      " [-0.68253631 -0.21686905 -0.40769328  0.59719476  0.6132289 ]\n",
      " [-0.02598207 -0.79010859 -0.58514363  1.11570934  0.53171016]\n",
      " [-0.0928353   0.98939488  0.01600223 -0.2885858  -0.30113186]\n",
      " [ 0.0834884  -0.1795091   0.0918187   0.32428411 -0.21110414]\n",
      " [ 0.22514371 -0.15751801 -0.01075847 -0.35310034  0.48898828]\n",
      " [ 0.7912184   0.62486306  0.53976326 -0.98903058 -0.80118419]\n",
      " [ 0.3943569  -0.13721604  0.22920444 -0.16360586 -0.37000955]\n",
      " [ 0.51547603  1.19935246  0.33495855 -0.97794005 -1.00305289]\n",
      " [ 0.61086278  0.02248284  0.02918483 -0.75624651  0.02808172]]\n",
      "2800 cost: 1.86532024953 x= [[-0.20053381  0.01835443  0.2241094   0.49665248 -0.20076767]\n",
      " [-0.6794397  -0.21289695 -0.41075436  0.58330899  0.63143131]\n",
      " [-0.03539345 -0.79548138 -0.58098761  1.11021475  0.5266349 ]\n",
      " [-0.11374536  0.97484927  0.00463488 -0.25125585 -0.31943756]\n",
      " [ 0.05866985 -0.19139908  0.07846103  0.3853211  -0.23145858]\n",
      " [ 0.22613365 -0.16230664 -0.00598657 -0.38295739  0.50127477]\n",
      " [ 0.78660003  0.63023525  0.53794791 -0.9924272  -0.81098776]\n",
      " [ 0.38076933 -0.14783849  0.22088401 -0.12862422 -0.36839051]\n",
      " [ 0.52941786  1.20985745  0.35056187 -1.02945587 -0.9975108 ]\n",
      " [ 0.61752895  0.02650624  0.03384437 -0.766895    0.02903261]]\n",
      "2900 cost: 1.76371957875 x= [[-0.20512535  0.08538973  0.18296497  0.46427382 -0.21877618]\n",
      " [-0.68106626 -0.18359284 -0.42625422  0.56839386  0.64177626]\n",
      " [-0.04369605 -0.8019224  -0.57646194  1.1052947   0.52240019]\n",
      " [-0.11717703  0.87203866  0.03861948 -0.20915817 -0.3146827 ]\n",
      " [ 0.06403105 -0.36280605  0.14592632  0.45553287 -0.21166552]\n",
      " [ 0.21572946 -0.10156458 -0.03400143 -0.41596429  0.49679044]\n",
      " [ 0.78119711  0.64197275  0.53281338 -0.99618182 -0.82146687]\n",
      " [ 0.38305893 -0.24174093  0.25519038 -0.08910986 -0.34687863]\n",
      " [ 0.52301017  1.32926849  0.3100824  -1.08623533 -1.01866874]\n",
      " [ 0.61985562  0.05164792  0.02739381 -0.7785058   0.0247905 ]]\n",
      "3000 cost: 1.84646649816 x= [[-0.2227151   0.11525034  0.15908585  0.4728741  -0.2423514 ]\n",
      " [-0.68734371 -0.16822407 -0.4354394   0.56869704  0.64851941]\n",
      " [-0.0509809  -0.80708903 -0.57268951  1.0999938   0.51867554]\n",
      " [-0.10338911  0.81755443  0.05238114 -0.21887274 -0.30125107]\n",
      " [ 0.09916818 -0.44851875  0.17662086  0.43336511 -0.17745397]\n",
      " [ 0.19400499 -0.07465808 -0.04710784 -0.41115232  0.48604069]\n",
      " [ 0.77498224  0.64951795  0.52946067 -0.9959506  -0.83158712]\n",
      " [ 0.40098169 -0.29014421  0.27037064 -0.09840403 -0.31888475]\n",
      " [ 0.49652783  1.38969708  0.29413891 -1.07893367 -1.04892053]\n",
      " [ 0.61792064  0.06512192  0.02563351 -0.77757388  0.01879999]]\n",
      "3100 cost: 1.94878551092 x= [[-0.23150107  0.08214409  0.15041841  0.50986905 -0.25308092]\n",
      " [-0.69043769 -0.17597273 -0.43906624  0.57961938  0.65842036]\n",
      " [-0.05778404 -0.81049549 -0.56956455  1.09453957  0.51503426]\n",
      " [-0.10004422  0.84400484  0.04835951 -0.26452291 -0.302593  ]\n",
      " [ 0.11467323 -0.39020347  0.17499037  0.3472344  -0.17067551]\n",
      " [ 0.1809684  -0.10538768 -0.04711748 -0.38007308  0.48606875]\n",
      " [ 0.77005955  0.6503704   0.52767073 -0.99295029 -0.83961303]\n",
      " [ 0.40868412 -0.26266587  0.26875005 -0.1415454  -0.3061968 ]\n",
      " [ 0.48371352  1.35138922  0.29971646 -1.02710019 -1.05973079]\n",
      " [ 0.618389    0.0590815   0.02798292 -0.76790963  0.01667211]]\n",
      "3200 cost: 1.95127171371 x= [[-0.21641859 -0.02286035  0.1803252   0.52810522 -0.23350428]\n",
      " [-0.68489834 -0.2098637  -0.42873523  0.58393941  0.67792766]\n",
      " [-0.06447441 -0.81199896 -0.56750529  1.08984288  0.51107859]\n",
      " [-0.12618326  0.96290039 -0.00312873 -0.28694363 -0.34083196]\n",
      " [ 0.07649379 -0.16748504  0.08797047  0.30340463 -0.23091051]\n",
      " [ 0.19032159 -0.2022805  -0.01251947 -0.36555718  0.51291678]\n",
      " [ 0.76784414  0.64362462  0.529711   -0.99166992 -0.84392092]\n",
      " [ 0.38827952 -0.14895673  0.22254386 -0.16249201 -0.32950929]\n",
      " [ 0.50781594  1.20057608  0.36307515 -1.00352318 -1.02408782]\n",
      " [ 0.6258236   0.03070085  0.04157159 -0.76361724  0.02367938]]\n",
      "3300 cost: 1.86132802176 x= [[-0.20894787 -0.01486968  0.17492863  0.49824491 -0.23399939]\n",
      " [-0.68206556 -0.20353356 -0.43103149  0.57125904  0.68904808]\n",
      " [-0.07043757 -0.81552624 -0.5649998   1.08652277  0.50786995]\n",
      " [-0.14204959  0.93992411 -0.00871689 -0.24862594 -0.35244057]\n",
      " [ 0.05587886 -0.19938704  0.08265283  0.36863744 -0.2442908 ]\n",
      " [ 0.19308495 -0.19515971 -0.01108086 -0.3949731   0.52042914]\n",
      " [ 0.76521523  0.64765154  0.52834729 -0.99513548 -0.84958149]\n",
      " [ 0.37706584 -0.16807125  0.21916084 -0.12659037 -0.32910493]\n",
      " [ 0.52012909  1.22348497  0.37021338 -1.0538054  -1.01994216]\n",
      " [ 0.63068124  0.03642094  0.04399836 -0.77383986  0.02450032]]\n",
      "3400 cost: 1.76085413702 x= [[-0.21332578  0.05605924  0.13768627  0.46581803 -0.24942462]\n",
      " [-0.68350852 -0.17471713 -0.44480166  0.55784985  0.69370376]\n",
      " [-0.07562052 -0.82014823 -0.56203218  1.08358444  0.50529551]\n",
      " [-0.14243419  0.83881211  0.02690624 -0.20725849 -0.34413029]\n",
      " [ 0.06232113 -0.37230953  0.15090738  0.43975204 -0.2226467 ]\n",
      " [ 0.18539834 -0.13038288 -0.03943098 -0.42643124  0.51347745]\n",
      " [ 0.76173687  0.65748592  0.52391719 -0.99880179 -0.85615007]\n",
      " [ 0.38009072 -0.26102548  0.2545058  -0.08772068 -0.31101889]\n",
      " [ 0.5140931   1.34284436  0.32657379 -1.10755341 -1.03932906]\n",
      " [ 0.63172926  0.06087027  0.03640225 -0.78467441  0.02072623]]\n",
      "3500 cost: 1.85275895602 x= [[-0.23015157  0.08195286  0.12046993  0.47712656 -0.2695314 ]\n",
      " [-0.68942664 -0.16252164 -0.45128518  0.560482    0.69570994]\n",
      " [-0.08007496 -0.82349557 -0.55974472  1.0800509   0.50305671]\n",
      " [-0.1265756   0.79533934  0.03827125 -0.22106007 -0.32888074]\n",
      " [ 0.09719028 -0.44243026  0.17529163  0.4122334  -0.18925698]\n",
      " [ 0.16664432 -0.10668383 -0.04995691 -0.41775595  0.50154596]\n",
      " [ 0.75732394  0.66260986  0.52151937 -0.99820794 -0.86265061]\n",
      " [ 0.39803002 -0.29966314  0.26689738 -0.10083126 -0.28743437]\n",
      " [ 0.48879403  1.39162937  0.3126483  -1.09332504 -1.06627184]\n",
      " [ 0.62879696  0.07145908  0.03454766 -0.78222354  0.01548254]]\n",
      "3600 cost: 1.95517835468 x= [[-0.23837224  0.05104217  0.11507078  0.51442276 -0.27778464]\n",
      " [-0.69225569 -0.1710396  -0.45347176  0.57269192  0.70108418]\n",
      " [-0.08429196 -0.82538888 -0.55790209  1.07627038  0.50079009]\n",
      " [-0.12117523  0.82456646  0.03560287 -0.26769933 -0.32772731]\n",
      " [ 0.11277412 -0.38300346  0.17423167  0.32613249 -0.18167278]\n",
      " [ 0.15617237 -0.13506616 -0.05013702 -0.3851619   0.49991013]\n",
      " [ 0.7540226   0.66186284  0.52033272 -0.9950662  -0.867497  ]\n",
      " [ 0.40592682 -0.27023094  0.26603164 -0.14483355 -0.27791479]\n",
      " [ 0.47681739  1.35174584  0.31579739 -1.03860174 -1.07516643]\n",
      " [ 0.62831314  0.0645451   0.03597394 -0.77183     0.0138105 ]]\n",
      "3700 cost: 1.94642958292 x= [[-0.22101451 -0.05312372  0.15022596  0.52838003 -0.25424291]\n",
      " [-0.68584879 -0.20606079 -0.44100693  0.57650679  0.71712052]\n",
      " [-0.08866203 -0.82548875 -0.55705474  1.07320654  0.49804987]\n",
      " [-0.14764898  0.94745232 -0.01749021 -0.28525288 -0.36585479]\n",
      " [ 0.07071781 -0.15665431  0.08259972  0.29264718 -0.24502121]\n",
      " [ 0.16944912 -0.23089171 -0.01359072 -0.37346584  0.52683579]\n",
      " [ 0.75344875  0.65358523  0.52315045 -0.99408798 -0.86878363]\n",
      " [ 0.3836289  -0.15337613  0.2177841  -0.16113423 -0.30611344]\n",
      " [ 0.50437236  1.19772114  0.38051775 -1.0194894  -1.0351636 ]\n",
      " [ 0.63540688  0.03503161  0.04941315 -0.76825968  0.02173392]]\n",
      "3800 cost: 1.8561014842 x= [[-0.21471724 -0.03443835  0.14410164  0.49743275 -0.25508919]\n",
      " [-0.68340796 -0.19718394 -0.44335206  0.56435574  0.72368206]\n",
      " [-0.0924404  -0.82795565 -0.5555302   1.07132499  0.49603717]\n",
      " [-0.15971306  0.91558024 -0.01757477 -0.24600989 -0.37237462]\n",
      " [ 0.05394    -0.20771732  0.08553466  0.3609836  -0.25226532]\n",
      " [ 0.17278988 -0.21369686 -0.0153972  -0.40292128  0.53081687]\n",
      " [ 0.75199263  0.65722771  0.52196301 -0.99754252 -0.87212404]\n",
      " [ 0.37455753 -0.18141725  0.21904765 -0.12431182 -0.30543275]\n",
      " [ 0.51478174  1.23309546  0.38043343 -1.06955037 -1.03320937]\n",
      " [ 0.63896644  0.04274541  0.05004136 -0.77830286  0.02217232]]\n",
      "3900 cost: 1.75922352277 x= [[-0.21947456  0.03996454  0.10877128  0.46562528 -0.26941837]\n",
      " [-0.68497076 -0.16829244 -0.45624644  0.55197492  0.7247203 ]\n",
      " [-0.09564567 -0.83148951 -0.55350232  1.0696771   0.49452314]\n",
      " [-0.15750142  0.81432937  0.0198116  -0.2058243  -0.36123212]\n",
      " [ 0.06233739 -0.38388242  0.1554675   0.43129748 -0.22822951]\n",
      " [ 0.16624783 -0.14553108 -0.04430835 -0.43296037  0.52197894]\n",
      " [ 0.74964553  0.66612198  0.51795368 -1.00102809 -0.87647326]\n",
      " [ 0.37873385 -0.27510438  0.25555208 -0.0864797  -0.28874676]\n",
      " [ 0.50805216  1.35414014  0.33424642 -1.12066258 -1.05242534]\n",
      " [ 0.63903904  0.0671048   0.04159793 -0.78848946  0.0184694 ]]\n",
      "4000 cost: 1.85959778427 x= [[-0.23546216  0.05970412  0.09702422  0.48013629 -0.28673784]\n",
      " [-0.69057735 -0.15941119 -0.46058455  0.55648119  0.72410299]\n",
      " [-0.09832544 -0.8336382  -0.55214669  1.06726991  0.49320792]\n",
      " [-0.14076029  0.78245841  0.02810211 -0.2239893  -0.34566355]\n",
      " [ 0.09633289 -0.43590626  0.17295434  0.39712133 -0.19669696]\n",
      " [ 0.14960555 -0.12727361 -0.05191566 -0.42061709  0.50999414]\n",
      " [ 0.74637517  0.66950475  0.51630933 -1.00000591 -0.88080436]\n",
      " [ 0.39632892 -0.3033517   0.26455169 -0.10365121 -0.26854105]\n",
      " [ 0.48393409  1.39005067  0.32381131 -1.09999872 -1.07645718]\n",
      " [ 0.63557366  0.0748048   0.04010358 -0.78466552  0.01382348]]\n",
      "4100 cost: 1.96201282917 x= [[-0.24325775  0.03015458  0.09340511  0.51789236 -0.29340296]\n",
      " [-0.69325597 -0.16840674 -0.461986    0.56947952  0.7267625 ]\n",
      " [-0.1009154  -0.83460704 -0.5510739   1.06457935  0.49180882]\n",
      " [-0.13413048  0.81353728  0.02666224 -0.27150603 -0.3430389 ]\n",
      " [ 0.11189939 -0.37570212  0.17283464  0.31051572 -0.18865313]\n",
      " [ 0.14073893 -0.15427942 -0.05233899 -0.38694104  0.50746528]\n",
      " [ 0.74407795  0.66787414  0.51550232 -0.99670245 -0.88379702]\n",
      " [ 0.40432968 -0.27276153  0.26438563 -0.1484083  -0.26094623]\n",
      " [ 0.47248828  1.34924832  0.32522097 -1.04325793 -1.08419699]\n",
      " [ 0.63450102  0.06735887  0.04089826 -0.77376421  0.01239918]]\n",
      "4200 cost: 1.94088955849 x= [[-0.22367448 -0.0735882   0.13247364  0.52725785 -0.26670062]\n",
      " [-0.68605109 -0.20417466 -0.44797565  0.57218621  0.74096836]\n",
      " [-0.1038421  -0.83384026 -0.55097139  1.06261509  0.48982099]\n",
      " [-0.16178878  0.93911405 -0.02817924 -0.28351973 -0.38198296]\n",
      " [ 0.06569507 -0.14681799  0.0768865   0.28791229 -0.25544155]\n",
      " [ 0.15714223 -0.24959037 -0.01391219 -0.37885756  0.53515742]\n",
      " [ 0.74459295  0.65873583  0.51889237 -0.99605772 -0.88325157]\n",
      " [ 0.37995399 -0.15387769  0.21403597 -0.15944155 -0.29291759]\n",
      " [ 0.50334964  1.19312661  0.39193038 -1.03007556 -1.04050771]\n",
      " [ 0.64160875  0.03712788  0.05445487 -0.77125187  0.02105565]]\n",
      "4300 cost: 1.84933840197 x= [[-0.21838795 -0.0459615   0.12476786  0.49535956 -0.26825599]\n",
      " [-0.6839872  -0.19289236 -0.45077731  0.56018012  0.74458662]\n",
      " [-0.10624019 -0.83570581 -0.55000933  1.06167179  0.48857384]\n",
      " [-0.17109261  0.89890327 -0.02336008 -0.24327966 -0.38472975]\n",
      " [ 0.0520248  -0.21492278  0.08752198  0.35888062 -0.25770144]\n",
      " [ 0.16053602 -0.22408829 -0.01870379 -0.40867302  0.53656852]\n",
      " [ 0.74382936  0.66245492  0.51772785 -0.99949466 -0.88530057]\n",
      " [ 0.37260361 -0.19022871  0.21947667 -0.12164296 -0.29140096]\n",
      " [ 0.51205706  1.23984433  0.38572791 -1.080575   -1.04076563]\n",
      " [ 0.64425926  0.04678913  0.05363611 -0.78129062  0.02106561]]\n",
      "4400 cost: 1.75993352827 x= [[-0.22418089  0.0317915   0.09014884  0.4648833  -0.28265692]\n",
      " [-0.68594106 -0.16353139 -0.46331759  0.54870475  0.74316612]\n",
      " [-0.1081846  -0.83859504 -0.54853345  1.06083889  0.48774537]\n",
      " [-0.16621734  0.79613632  0.0157765  -0.20495382 -0.37088253]\n",
      " [ 0.0635266  -0.39574807  0.15959252  0.42663069 -0.23042038]\n",
      " [ 0.15392728 -0.15277408 -0.04834205 -0.43703577  0.52593424]\n",
      " [ 0.74209895  0.67096318  0.51395578 -1.00274175 -0.88843606]\n",
      " [ 0.37849385 -0.28581764  0.25722384 -0.08550982 -0.27461885]\n",
      " [ 0.50358371  1.36381803  0.33732358 -1.12871765 -1.06112155]\n",
      " [ 0.64346716  0.07146585  0.04453357 -0.7908097   0.01714073]]\n",
      "4500 cost: 1.86616075832 x= [[-0.23921868  0.04474591  0.08276867  0.48244313 -0.29764045]\n",
      " [-0.69122387 -0.15778817 -0.46598913  0.55469227  0.741191  ]\n",
      " [-0.10975843 -0.83993694 -0.54775518  1.05913552  0.48699923]\n",
      " [-0.14943094  0.77527034  0.02098475 -0.22710028 -0.35588002]\n",
      " [ 0.09608223 -0.42978837  0.17062164  0.38599111 -0.20135574]\n",
      " [ 0.13895091 -0.14065426 -0.05320148 -0.42150574  0.51452875]\n",
      " [ 0.73957359  0.67304586  0.51289392 -1.00131927 -0.89144005]\n",
      " [ 0.39541019 -0.30412138  0.26292944 -0.10640249 -0.25716363]\n",
      " [ 0.48079661  1.38717612  0.33064197 -1.10252962 -1.08248075]\n",
      " [ 0.63978592  0.07648455  0.04356546 -0.78583136  0.01301595]]\n",
      "4600 cost: 1.96832956211 x= [[-0.24660062  0.01547388  0.08033857  0.52072639 -0.30313385]\n",
      " [-0.69377946 -0.1672851  -0.46689253  0.56820846  0.74227688]\n",
      " [-0.11132929 -0.84032572 -0.54714091  1.05714449  0.48614364]\n",
      " [-0.14219578  0.80829318  0.02023949 -0.27540846 -0.3525819 ]\n",
      " [ 0.11136201 -0.36776209  0.17090751  0.29858992 -0.19340214]\n",
      " [ 0.13118817 -0.16737826 -0.05365083 -0.38699538  0.51167228]\n",
      " [ 0.73790976  0.67087406  0.51235077 -0.99786701 -0.89333385]\n",
      " [ 0.40333951 -0.27217004  0.26307029 -0.15185993 -0.2509228 ]\n",
      " [ 0.46985359  1.34495363  0.33117728 -1.04427125 -1.08928034]\n",
      " [ 0.6383815   0.06853966  0.04400447 -0.77456107  0.01177356]]\n",
      "4700 cost: 1.93461051704 x= [[-0.22479671 -0.08851455  0.12260638  0.52545744 -0.27370044]\n",
      " [-0.68579556 -0.20370937 -0.45165998  0.56953078  0.75566681]\n",
      " [-0.11336685 -0.83901032 -0.54749624  1.05592057  0.48461402]\n",
      " [-0.17161328  0.93623222 -0.03661696 -0.28167978 -0.39299965]\n",
      " [ 0.06074812 -0.13611777  0.07055717  0.28682969 -0.26401787]\n",
      " [ 0.15026825 -0.26287053 -0.01331764 -0.38283236  0.54058867]\n",
      " [ 0.73917717  0.66120975  0.51618771 -0.99757929 -0.89163907]\n",
      " [ 0.37671432 -0.15144135  0.21047642 -0.15746131 -0.28609544]\n",
      " [ 0.50401041  1.18676362  0.40037431 -1.03761425 -1.04217173]\n",
      " [ 0.64573869  0.03771761  0.05787337 -0.77326043  0.02109027]]\n",
      "4800 cost: 1.84112706709 x= [[-0.22049899 -0.05263843  0.11258813  0.49264663 -0.27630216]\n",
      " [-0.68411516 -0.18996501 -0.45524229  0.55745348  0.75727654]\n",
      " [-0.11489074 -0.84056253 -0.54683535  1.05558562  0.483863  ]\n",
      " [-0.17871875  0.88749285 -0.02704128 -0.24036134 -0.39264032]\n",
      " [ 0.04987842 -0.22086641  0.08899568  0.36021562 -0.26177382]\n",
      " [ 0.15333756 -0.22977396 -0.02114844 -0.41321208  0.53989201]\n",
      " [ 0.73880434  0.66529315  0.51492309 -1.00101692 -0.89300293]\n",
      " [ 0.3708939  -0.19617101  0.22006789 -0.11863401 -0.28329188]\n",
      " [ 0.511055    1.2447174   0.38837113 -1.08896562 -1.0447939 ]\n",
      " [ 0.64770502  0.04941142  0.05576043 -0.78341048  0.02062655]]\n",
      "4900 cost: 1.76352665173 x= [[-0.22786497  0.02804046  0.07815429  0.46422611 -0.291565  ]\n",
      " [-0.6866634  -0.16001663 -0.46767     0.54694432  0.75406977]\n",
      " [-0.11602394 -0.84306923 -0.54567934  1.05524866  0.48348416]\n",
      " [-0.17098611  0.7827548   0.01353758 -0.20469131 -0.37607984]\n",
      " [ 0.06546356 -0.40647027  0.16298619  0.42364568 -0.23059205]\n",
      " [ 0.14580789 -0.15575551 -0.0514443  -0.43944846  0.52743021]\n",
      " [ 0.73736185  0.67369245  0.51128808 -1.00397788 -0.89551647]\n",
      " [ 0.37898014 -0.29395609  0.25887134 -0.08497829 -0.26543783]\n",
      " [ 0.50001672  1.37183308  0.33824426 -1.13343476 -1.06716942]\n",
      " [ 0.64608362  0.0745495   0.04617742 -0.79215837  0.01629381]]\n",
      "5000 cost: 1.87236972208 x= [[-0.24193509  0.03503612  0.07386821  0.48430145 -0.30467041]\n",
      " [-0.69162364 -0.15685811 -0.4691826   0.55404027  0.75143644]\n",
      " [-0.11691191 -0.84387202 -0.54524943  1.05398175  0.48308775]\n",
      " [-0.15462312  0.77096413  0.01637733 -0.23005116 -0.36196679]\n",
      " [ 0.09631615 -0.42536582  0.16915266  0.37762983 -0.20399417]\n",
      " [ 0.13219043 -0.14907499 -0.05422361 -0.42140223  0.51681069]\n",
      " [ 0.73533281  0.67483805  0.51063793 -1.00222894 -0.89769918]\n",
      " [ 0.395053   -0.30402764  0.26206205 -0.10888837 -0.2501583 ]\n",
      " [ 0.47860245  1.38470797  0.33452586 -1.10291469 -1.08624883]\n",
      " [ 0.64237402  0.0773786   0.04565692 -0.7862872   0.01260324]]\n",
      "5100 cost: 1.97357241362 x= [[-0.24873099  0.0043714   0.07279965  0.5231254  -0.30897774]\n",
      " [-0.69399838 -0.16723102 -0.46956712  0.56792132  0.75173698]\n",
      " [-0.11785058 -0.84386825 -0.54491583  1.05243345  0.48256496]\n",
      " [-0.14740635  0.80721791  0.01529703 -0.27908297 -0.35882977]\n",
      " [ 0.11071309 -0.35863043  0.16828753  0.28929151 -0.19710602]\n",
      " [ 0.1253973  -0.17709427 -0.05409279 -0.3862066   0.51420426]\n",
      " [ 0.73409212  0.67221213  0.51033216 -0.99864864 -0.89890505]\n",
      " [ 0.40257178 -0.2693914   0.26164235 -0.15500333 -0.24526554]\n",
      " [ 0.46844724  1.33919275  0.33551928 -1.04344156 -1.09179174]\n",
      " [ 0.64085625  0.06864878  0.04607046 -0.77473421  0.0115968 ]]\n",
      "5200 cost: 1.92777342156 x= [[-0.22477713 -0.10026579  0.11774784  0.52336197 -0.27714623]\n",
      " [-0.68526305 -0.20425016 -0.45333658  0.56777082  0.76489982]\n",
      " [-0.11934645 -0.84219969 -0.54555364  1.05171182  0.48130942]\n",
      " [-0.17886448  0.93727568 -0.04366122 -0.27971434 -0.40104256]\n",
      " [ 0.05565855 -0.12412941  0.06364723  0.28795386 -0.27165567]\n",
      " [ 0.14682477 -0.27317267 -0.011925   -0.38597952  0.54454809]\n",
      " [ 0.73589862  0.66220697  0.51452427 -0.99872637 -0.8964639 ]\n",
      " [ 0.37365559 -0.14693551  0.20682    -0.15528938 -0.28322789]\n",
      " [ 0.50581026  1.17895882  0.40736339 -1.04338084 -1.04150812]\n",
      " [ 0.64859294  0.03731478  0.06034621 -0.77468427  0.02151953]]\n",
      "5300 cost: 1.83216139215 x= [[-0.22158272 -0.05621681  0.10477436  0.4897429  -0.28114837]\n",
      " [-0.68400949 -0.18787966 -0.45796506  0.55553852  0.76501303]\n",
      " [-0.12031201 -0.84362317 -0.54503328  1.05177034  0.48088759]\n",
      " [-0.18396987  0.8794239  -0.0291968  -0.2373832  -0.39780797]\n",
      " [ 0.047667   -0.2260866   0.09037413  0.36342703 -0.26489501]\n",
      " [ 0.14923052 -0.2325872  -0.02301661 -0.41696038  0.54188177]\n",
      " [ 0.73571911  0.66684639  0.51305947 -1.00217874 -0.89750829]\n",
      " [ 0.36938536 -0.20049581  0.22078156 -0.11551468 -0.27870505]\n",
      " [ 0.51103904  1.2486172   0.38943784 -1.095684   -1.04679029]\n",
      " [ 0.64998074  0.05119793  0.05697419 -0.78498952  0.02052092]]\n",
      "5400 cost: 1.76922153878 x= [[-0.23080575  0.02678984  0.07036474  0.4638495  -0.29769168]\n",
      " [-0.68725347 -0.15739093 -0.47036612  0.54605925  0.76044961]\n",
      " [-0.12091805 -0.84590086 -0.54406135  1.0517222   0.48080773]\n",
      " [-0.17333081  0.77283385  0.01241149 -0.20490575 -0.37861769]\n",
      " [ 0.06781295 -0.41579945  0.16577066  0.42138114 -0.22958171]\n",
      " [ 0.14030463 -0.15643331 -0.05378943 -0.44075369  0.52762279]\n",
      " [ 0.73436701  0.6752566   0.50951009 -1.00482993 -0.89973817]\n",
      " [ 0.37989581 -0.30025358  0.26033911 -0.08486183 -0.25927647]\n",
      " [ 0.49702061  1.37847943  0.33812721 -1.13597708 -1.07160038]\n",
      " [ 0.64757064  0.07678324  0.04706889 -0.79289039  0.01569571]]\n",
      "5500 cost: 1.87880815516 x= [[-0.24403477  0.02917519  0.06797245  0.48600092 -0.30940617]\n",
      " [-0.69193627 -0.1561686  -0.47117996  0.55400937  0.75750584]\n",
      " [-0.12138128 -0.84634923 -0.54383389  1.05072257  0.48062733]\n",
      " [-0.15751874  0.76785438  0.01382977 -0.23287928 -0.36533967]\n",
      " [ 0.09709202 -0.42321041  0.16898633  0.37089531 -0.20500487]\n",
      " [ 0.12773991 -0.15398389 -0.05528891 -0.42069452  0.51771882]\n",
      " [ 0.73266838  0.67575514  0.50912112 -1.00281939 -0.90141385]\n",
      " [ 0.39517596 -0.30414169  0.26199446 -0.11123756 -0.24559019]\n",
      " [ 0.4768087   1.38344256  0.3362268  -1.10200877 -1.08894043]\n",
      " [ 0.64391151  0.07797799  0.04682141 -0.78631838  0.01233255]]\n",
      "5600 cost: 1.97718578701 x= [[-0.24987084 -0.00478061  0.06891626  0.52506437 -0.31217955]\n",
      " [-0.69399169 -0.16796048 -0.47083149  0.56806089  0.75762217]\n",
      " [-0.12193661 -0.84604792 -0.54368633  1.04945569  0.48029703]\n",
      " [-0.15105024  0.80920767  0.01108086 -0.28221961 -0.36330636]\n",
      " [ 0.109619   -0.34803621  0.16477267  0.28221216 -0.20056339]\n",
      " [ 0.12208374 -0.18505718 -0.05371646 -0.38516926  0.51601668]\n",
      " [ 0.7317473   0.67262808  0.50908424 -0.99915814 -0.90213627]\n",
      " [ 0.40174697 -0.26496891  0.25983779 -0.15762894 -0.24245882]\n",
      " [ 0.46804448  1.33212896  0.33932642 -1.04195664 -1.09249042]\n",
      " [ 0.64249906  0.0680221   0.04757583 -0.77462257  0.01170486]]\n",
      "5700 cost: 1.9210018652 x= [[-0.22415992 -0.10938397  0.11569573  0.52123792 -0.27857788]\n",
      " [-0.68464317 -0.20519923 -0.45392854  0.56652029  0.77076145]\n",
      " [-0.1231016  -0.84416706 -0.54449149  1.04907633  0.47920872]\n",
      " [-0.18429042  0.9400401  -0.04937702 -0.27772022 -0.40698189]\n",
      " [ 0.05087994 -0.11273998  0.05710061  0.29024545 -0.278171  ]\n",
      " [ 0.14534131 -0.28110641 -0.01025054 -0.38856246  0.54751657]\n",
      " [ 0.73392759  0.66247519  0.51351901 -0.99957908 -0.899225  ]\n",
      " [ 0.3709193  -0.14195931  0.20342793 -0.15309222 -0.28246865]\n",
      " [ 0.50803125  1.17130467  0.41308617 -1.0479829  -1.03982147]\n",
      " [ 0.65059306  0.03650744  0.06215556 -0.77574031  0.02207863]]\n",
      "5800 cost: 1.82316243282 x= [[-0.222123   -0.05741766  0.09946552  0.48689662 -0.28414605]\n",
      " [-0.68383327 -0.1861742  -0.45972925  0.55410029  0.769706  ]\n",
      " [-0.12370768 -0.84557186 -0.54401881  1.04939087  0.47900949]\n",
      " [-0.18754781  0.87296436 -0.03007307 -0.23445466 -0.40108423]\n",
      " [ 0.04576278 -0.2317943   0.09219319  0.36754205 -0.26701856]\n",
      " [ 0.14689597 -0.23327992 -0.02465855 -0.42013969  0.54300658]\n",
      " [ 0.73382185  0.6677568   0.51179797 -1.00305883 -0.90015736]\n",
      " [ 0.36818424 -0.20437993  0.2217853  -0.11245556 -0.27603379]\n",
      " [ 0.5113873   1.25263252  0.38930062 -1.10125007 -1.04785025]\n",
      " [ 0.65147601  0.05261268  0.05757436 -0.78621916  0.02052983]]\n",
      "5900 cost: 1.7757401917 x= [[-0.23326482  0.02749186  0.06500246  0.46362658 -0.30213314]\n",
      " [-0.68779048 -0.15519147 -0.47214568  0.54562853  0.76407929]\n",
      " [-0.12396725 -0.84771485 -0.54315083  1.04950287  0.47913171]\n",
      " [-0.17412258  0.76465486  0.01227236 -0.2052679  -0.37943744]\n",
      " [ 0.07049532 -0.42508587  0.16862342  0.41973355 -0.22765689]\n",
      " [ 0.13638935 -0.15538629 -0.05578287 -0.4414619   0.52704795]\n",
      " [ 0.73244949  0.67623559  0.50830074 -1.0054085  -0.90229178]\n",
      " [ 0.38111764 -0.30590517  0.2618881  -0.08490707 -0.25484356]\n",
      " [ 0.49431814  1.38490947  0.33716207 -1.13734064 -1.07519231]\n",
      " [ 0.64833927  0.07861826  0.04745001 -0.79328103  0.0151939 ]]\n",
      "6000 cost: 1.88578953009 x= [[-0.2457778   0.02602222  0.06375417  0.48773042 -0.31279043]\n",
      " [-0.69223754 -0.15553045 -0.47254507  0.55433812  0.76102182]\n",
      " [-0.12416872 -0.84792202 -0.5430396   1.04866001  0.47908356]\n",
      " [-0.15888866  0.76517573  0.01285591 -0.23567979 -0.36692235]\n",
      " [ 0.09837485 -0.42310595  0.17008233  0.36499471 -0.20476248]\n",
      " [ 0.12465098 -0.15648655 -0.05650856 -0.41956012  0.51778003]\n",
      " [ 0.73097483  0.67624901  0.50807635 -1.00316696 -0.90364722]\n",
      " [ 0.39568574 -0.30478193  0.26262557 -0.11359028 -0.24236098]\n",
      " [ 0.47514049  1.3834279   0.3363522  -1.10022971 -1.09116623]\n",
      " [ 0.64476802  0.07849719  0.04736489 -0.7860804   0.01208755]]\n",
      "6100 cost: 1.97840147854 x= [[-0.25016433 -0.01276785  0.06749082  0.52628512 -0.3135359 ]\n",
      " [-0.69379074 -0.16921017 -0.47119061  0.5682434   0.76142149]\n",
      " [-0.12450226 -0.84737253 -0.54303094  1.0475816   0.47885129]\n",
      " [-0.15395049  0.81323671  0.00713194 -0.28437482 -0.36692677]\n",
      " [ 0.10782804 -0.3363608   0.16030437  0.27759012 -0.20425904]\n",
      " [ 0.12047494 -0.19203296 -0.05262247 -0.38442777  0.51763366]\n",
      " [ 0.73033461  0.67254626  0.50836126 -0.99951072 -0.9039761 ]\n",
      " [ 0.40066373 -0.25948559  0.25755784 -0.15936286 -0.24158646]\n",
      " [ 0.46854786  1.32419055  0.34315312 -1.04084298 -1.0918274 ]\n",
      " [ 0.64367589  0.06692999  0.04879937 -0.77449349  0.01201504]]\n",
      "6200 cost: 1.91492904475 x= [[-0.22348379 -0.11556147  0.11479635  0.519257   -0.27919054]\n",
      " [-0.68410885 -0.20594666 -0.45409578  0.565587    0.77442744]\n",
      " [-0.1254572  -0.84539655 -0.54391987  1.0474316   0.47787662]\n",
      " [-0.18813253  0.94229476 -0.05347574 -0.27581402 -0.41105754]\n",
      " [ 0.04709287 -0.10461425  0.05213812  0.29299563 -0.28294906]\n",
      " [ 0.14474903 -0.28642465 -0.00890749 -0.39070137  0.54953273]\n",
      " [ 0.73273406  0.66249075  0.51289075 -1.00020805 -0.90081998]\n",
      " [ 0.36880012 -0.13825211  0.20088403 -0.15101959 -0.2824352 ]\n",
      " [ 0.50996439  1.16576921  0.41716503 -1.05171491 -1.03823993]\n",
      " [ 0.65195123  0.03585015  0.06338309 -0.77654791  0.02255486]]\n",
      "6300 cost: 1.81445169283 x= [[-0.22245429 -0.05656885  0.09547777  0.48417058 -0.28616966]\n",
      " [-0.68368381 -0.18451226 -0.46101578  0.55293644  0.77249477]\n",
      " [-0.12583226 -0.84683552 -0.54344753  1.04791553  0.4778273 ]\n",
      " [-0.18987205  0.8668438  -0.02981621 -0.23157673 -0.40295341]\n",
      " [ 0.04442491 -0.23904495  0.09483871  0.37211362 -0.26807052]\n",
      " [ 0.14550751 -0.23217772 -0.0263454  -0.42293296  0.54348661]\n",
      " [ 0.73264607  0.66838863  0.51091064 -1.00373213 -0.90173596]\n",
      " [ 0.3673683  -0.20868117  0.22323429 -0.10948843 -0.27428158]\n",
      " [ 0.51170253  1.25761003  0.38809978 -1.10605882 -1.04866319]\n",
      " [ 0.65243192  0.05397463  0.05773005 -0.78723095  0.0205276 ]]\n",
      "6400 cost: 1.78253450502 x= [[-0.23545586  0.02988711  0.06095956  0.46343269 -0.30561264]\n",
      " [-0.68832751 -0.15309593 -0.4734545   0.54540856  0.76601426]\n",
      " [-0.12586064 -0.84890102 -0.54263751  1.04810897  0.47808873]\n",
      " [-0.17386217  0.75700939  0.01302148 -0.20555987 -0.37907055]\n",
      " [ 0.07353275 -0.4353804   0.17196431  0.41869715 -0.22491899]\n",
      " [ 0.13339794 -0.15287123 -0.05770419 -0.44189851  0.52596496]\n",
      " [ 0.73119463  0.67695603  0.50744703 -1.00580213 -0.90387604]\n",
      " [ 0.3826043  -0.31173564  0.26370001 -0.08493253 -0.25132556]\n",
      " [ 0.49169499  1.39195145  0.33541951 -1.13815392 -1.07845337]\n",
      " [ 0.64863778  0.08035454  0.04746395 -0.79350159  0.01469958]]\n",
      "6500 cost: 1.89312439537 x= [[-0.24727323  0.0245985   0.06054249  0.4895334  -0.31532782]\n",
      " [-0.69254246 -0.15491859 -0.47355836  0.55486827  0.76299353]\n",
      " [-0.12590389 -0.84892824 -0.54259588  1.04735117  0.47811777]\n",
      " [-0.1593006   0.76275791  0.01293815 -0.23845927 -0.36739141]\n",
      " [ 0.09998333 -0.42433437  0.17206014  0.35956139 -0.20371104]\n",
      " [ 0.12240234 -0.15749125 -0.05783255 -0.41814741  0.51736465]\n",
      " [ 0.72988424  0.67653649  0.50734066 -1.00334014 -0.90501142]\n",
      " [ 0.39643916 -0.30581379  0.26372311 -0.11597304 -0.23991939]\n",
      " [ 0.47354186  1.38426478  0.33548111 -1.09789715 -1.09314364]\n",
      " [ 0.64519689  0.07898345  0.04751842 -0.78567998  0.01183746]]\n",
      "6600 cost: 1.97732101805 x= [[-0.24981084 -0.01992802  0.06765647  0.52677128 -0.31365515]\n",
      " [-0.69344307 -0.17075744 -0.47098946  0.56831065  0.7639936 ]\n",
      " [-0.12611883 -0.84816172 -0.54270162  1.04640844  0.47792229]\n",
      " [-0.15650105  0.81844626  0.00330002 -0.28549048 -0.37011794]\n",
      " [ 0.10539461 -0.32420636  0.15511615  0.27521218 -0.20826361]\n",
      " [ 0.12000851 -0.19833503 -0.05100421 -0.38416499  0.5192494 ]\n",
      " [ 0.72951022  0.67221205  0.50798778 -0.99977705 -0.90498901]\n",
      " [ 0.39930675 -0.25347206  0.25489798 -0.16016764 -0.24197603]\n",
      " [ 0.46975723  1.31586859  0.34712287 -1.04046964 -1.09022557]\n",
      " [ 0.64458808  0.06558295  0.0498724  -0.77445947  0.01245687]]\n",
      "6700 cost: 1.90947237696 x= [[-0.22301423 -0.11874935  0.11412031  0.51737433 -0.2796474 ]\n",
      " [-0.68373225 -0.20615995 -0.4542007   0.56482193  0.77663373]\n",
      " [-0.1269319  -0.84618691 -0.5436139   1.04641167  0.47703685]\n",
      " [-0.19064804  0.94280573 -0.05591995 -0.2739101  -0.41352063]\n",
      " [ 0.04452908 -0.10110801  0.04928912  0.29607389 -0.28581122]\n",
      " [ 0.14446419 -0.28908312 -0.00820286 -0.39258889  0.55066895]\n",
      " [ 0.73199806  0.6625057   0.51246465 -1.00068377 -0.90176634]\n",
      " [ 0.36738073 -0.13671758  0.19944429 -0.14900607 -0.28239087]\n",
      " [ 0.51130363  1.1633484   0.41948996 -1.05495349 -1.03731184]\n",
      " [ 0.65282699  0.03564121  0.06410517 -0.77721793  0.02285409]]\n",
      "6800 cost: 1.80624914746 x= [[-0.22277889 -0.05397665  0.09212207  0.4815913  -0.28774242]\n",
      " [-0.68360998 -0.18271725 -0.46209517  0.55193832  0.77407392]\n",
      " [-0.1271587  -0.84767811 -0.54312041  1.04700714  0.47708761]\n",
      " [-0.19122768  0.86037757 -0.02859163 -0.22876641 -0.40374318]\n",
      " [ 0.04377127 -0.24828213  0.09843899  0.37686007 -0.26807131]\n",
      " [ 0.14457099 -0.22955805 -0.02821754 -0.42544427  0.54345437]\n",
      " [ 0.73190452  0.66893104  0.5102513  -1.00425588 -0.90270174]\n",
      " [ 0.36696273 -0.21380353  0.22517802 -0.10663704 -0.27287865]\n",
      " [ 0.51177044  1.26391106  0.38597019 -1.11033644 -1.04960018]\n",
      " [ 0.65300557  0.05544798  0.05755648 -0.78810066  0.02045432]]\n",
      "6900 cost: 1.78939967605 x= [[-0.23741609  0.03314528  0.05781967  0.46330574 -0.3084327 ]\n",
      " [-0.68885445 -0.15114178 -0.47446043  0.54530928  0.76694474]\n",
      " [-0.12703212 -0.84968765 -0.54234771  1.0472327   0.47744383]\n",
      " [-0.17299767  0.74996754  0.0142405  -0.20582051 -0.37803422]\n",
      " [ 0.07669447 -0.44584124  0.17543343  0.41797877 -0.22178248]\n",
      " [ 0.13101866 -0.14964355 -0.05949074 -0.44214215  0.52464949]\n",
      " [ 0.73035604  0.67753082  0.50683501 -1.0060633  -0.90488488]\n",
      " [ 0.38420176 -0.31745677  0.26557171 -0.08497933 -0.24841437]\n",
      " [ 0.48918783  1.39907891  0.33334351 -1.13859497 -1.08144789]\n",
      " [ 0.64865472  0.08197331  0.04727852 -0.79361509  0.01421313]]\n",
      "7000 cost: 1.90043297631 x= [[-0.24853571  0.02397394  0.05808837  0.49141423 -0.3172441 ]\n",
      " [-0.69283341 -0.15443018 -0.47432343  0.55551097  0.76406307]\n",
      " [-0.12698246 -0.8495717  -0.54235041  1.04651617  0.47751448]\n",
      " [-0.15918935  0.76088009  0.01353075 -0.24121941 -0.3672798 ]\n",
      " [ 0.10165892 -0.42573871  0.17429883  0.35439261 -0.20234251]\n",
      " [ 0.1207307  -0.1578412  -0.05908749 -0.41655367  0.51676522]\n",
      " [ 0.72917485  0.67669897  0.50682177 -1.0033916  -0.90585524]\n",
      " [ 0.39727072 -0.30676577  0.26494947 -0.1183907  -0.23804798]\n",
      " [ 0.47207988  1.38520277  0.3342146  -1.09521449 -1.09485033]\n",
      " [ 0.64537643  0.07936675  0.04747321 -0.78518267  0.01159963]]\n",
      "7100 cost: 1.97523719908 x= [[ -2.49052370e-01  -2.67640516e-02   6.88505839e-02   5.26987098e-01\n",
      "   -3.12987262e-01]\n",
      " [ -6.93014306e-01  -1.72564915e-01  -4.70442606e-01   5.68351607e-01\n",
      "    7.65830577e-01]\n",
      " [ -1.27141662e-01  -8.48610123e-01  -5.42554955e-01   1.04567241e+00\n",
      "    4.77319026e-01]\n",
      " [ -1.58812538e-01   8.24657110e-01  -4.91194136e-04  -2.86152255e-01\n",
      "   -3.73057672e-01]\n",
      " [  1.02595188e-01  -3.11297787e-01   1.49401008e-01   2.73850791e-01\n",
      "   -2.12475441e-01]\n",
      " [  1.20238510e-01  -2.04413780e-01  -4.90120290e-02  -3.84048481e-01\n",
      "    5.20903747e-01]\n",
      " [  7.29050635e-01   6.71729731e-01   5.07846015e-01  -9.99962385e-01\n",
      "   -9.05512198e-01]\n",
      " [  3.97794773e-01  -2.46907256e-01   2.51951637e-01  -1.60595867e-01\n",
      "   -2.43142798e-01]\n",
      " [  4.71383603e-01   1.30701328e+00   3.51259702e-01  -1.04030801e+00\n",
      "   -1.08803948e+00]\n",
      " [  6.45329783e-01   6.40207235e-02   5.08671902e-02  -7.74440729e-01\n",
      "    1.29740661e-02]]\n",
      "7200 cost: 1.90428885099 x= [[-0.22281769 -0.11926067  0.11325695  0.51548279 -0.28023305]\n",
      " [-0.68351608 -0.20575298 -0.45439974  0.5641172   0.77787036]\n",
      " [-0.12785257 -0.84671755 -0.54344746  1.04578638  0.47651251]\n",
      " [-0.19213331  0.94122344 -0.05690725 -0.27187891 -0.41468951]\n",
      " [ 0.04308888 -0.10230061  0.04850102  0.29955461 -0.2869487 ]\n",
      " [ 0.14422336 -0.28935651 -0.00818374 -0.39440277  0.55107179]\n",
      " [ 0.73153146  0.66262532  0.512142   -1.0010633  -0.90235507]\n",
      " [ 0.36658462 -0.13750587  0.19907436 -0.1469289  -0.28204626]\n",
      " [ 0.51202658  1.16412715  0.42023586 -1.05802946 -1.03716376]\n",
      " [ 0.65335297  0.03596022  0.06441709 -0.77783577  0.02296407]]\n",
      "7300 cost: 1.79943090438 x= [[-0.22328818 -0.04998107  0.08904069  0.47932429 -0.28922091]\n",
      " [-0.68365995 -0.18073086 -0.46310209  0.55110462  0.7748518 ]\n",
      " [-0.12798171 -0.84826132 -0.54292506  1.04644946  0.4766313 ]\n",
      " [-0.1917203   0.85331323 -0.02659996 -0.22624281 -0.40361913]\n",
      " [ 0.04400439 -0.25945634  0.10291047  0.38124885 -0.26697535]\n",
      " [ 0.14372746 -0.22571827 -0.03030549 -0.42759019  0.54294998]\n",
      " [ 0.73141327  0.66947121  0.50973044 -1.00465646 -0.90332607]\n",
      " [ 0.36705107 -0.21981648  0.22756711 -0.10410382 -0.27144916]\n",
      " [ 0.51137041  1.2715263   0.38309258 -1.11397081 -1.05092513]\n",
      " [ 0.65327814  0.05708841  0.05714432 -0.78882415  0.02027013]]\n",
      "7400 cost: 1.79606106007 x= [[-0.23906489  0.03614303  0.05555219  0.46337239 -0.31063377]\n",
      " [-0.68932278 -0.14956848 -0.47517827  0.54532383  0.76734152]\n",
      " [-0.12775573 -0.85020466 -0.54218763  1.04667831  0.47704547]\n",
      " [-0.17195016  0.7443264   0.01537317 -0.20622129 -0.37683891]\n",
      " [ 0.07958361 -0.4545827   0.17828707  0.4171463  -0.21885141]\n",
      " [ 0.12916195 -0.14671948 -0.06090074 -0.44213951  0.52339749]\n",
      " [ 0.72979199  0.67795624  0.50641147 -1.00622029 -0.90553369]\n",
      " [ 0.38568253 -0.32216611  0.2671093  -0.08520515 -0.24610716]\n",
      " [ 0.48699343  1.40502321  0.33155293 -1.13859743 -1.08397293]\n",
      " [ 0.64854244  0.08327744  0.04706549 -0.7936251   0.01378641]]\n",
      "7500 cost: 1.90764739607 x= [[-0.24958201  0.02341497  0.05629198  0.49344871 -0.318667  ]\n",
      " [-0.69309736 -0.15417919 -0.47487819  0.55624715  0.76462433]\n",
      " [-0.1276519  -0.84997674 -0.54221753  1.04597645  0.47713759]\n",
      " [-0.15882486  0.75989988  0.01421754 -0.24407102 -0.36693609]\n",
      " [ 0.10324244 -0.42625879  0.17628491  0.34917942 -0.2010014 ]\n",
      " [ 0.11947206 -0.15819345 -0.06012156 -0.41476248  0.51616407]\n",
      " [ 0.72870883  0.67675906  0.5064629  -1.00335084 -0.90638138]\n",
      " [ 0.39807851 -0.30715767  0.26603254 -0.12094173 -0.23663911]\n",
      " [ 0.4707958   1.38553357  0.33300108 -1.09217653 -1.09624074]\n",
      " [ 0.64541787  0.07955548  0.04736284 -0.78460153  0.0113941 ]]\n",
      "7600 cost: 1.97327139943 x= [[-0.24798458 -0.03420675  0.07093281  0.52743307 -0.31169037]\n",
      " [-0.6925214  -0.1748286  -0.46960223  0.56850645  0.76727301]\n",
      " [-0.12779433 -0.84882702 -0.54251345  1.04520286  0.47691843]\n",
      " [-0.16103218  0.83253049 -0.00456133 -0.28700837 -0.37601217]\n",
      " [ 0.09946267 -0.296073    0.14280137  0.27224732 -0.21711634]\n",
      " [ 0.12094477 -0.21110954 -0.04655208 -0.3836686   0.52272378]\n",
      " [ 0.72882308  0.67109156  0.5078791  -1.00005281 -0.90573166]\n",
      " [ 0.39612961 -0.23904403  0.24853141 -0.16124971 -0.24494072]\n",
      " [ 0.47334252  1.29657451  0.35589369 -1.03968599 -1.08529019]\n",
      " [ 0.64597892  0.06207866  0.05189081 -0.77431837  0.01357376]]\n",
      "7700 cost: 1.89922562128 x= [[-0.22291121 -0.11747085  0.11202104  0.51353504 -0.28107597]\n",
      " [-0.68344925 -0.20473536 -0.45475915  0.56342046  0.77845152]\n",
      " [-0.1284242  -0.84709711 -0.54335049  1.04541006  0.47619108]\n",
      " [-0.19281119  0.93754882 -0.05665468 -0.26967912 -0.41482462]\n",
      " [ 0.04264222 -0.1078414   0.04958011  0.30342559 -0.28659833]\n",
      " [ 0.14389055 -0.28757338 -0.0088202  -0.39622784  0.55086785]\n",
      " [ 0.73122022  0.66288707  0.51186443 -1.00138342 -0.90275205]\n",
      " [ 0.36632881 -0.1404939   0.19967176 -0.14474156 -0.28128917]\n",
      " [ 0.51216608  1.16787798  0.41961219 -1.06110392 -1.03778963]\n",
      " [ 0.65362118  0.03680121  0.06439779 -0.77844407  0.02289624]]\n",
      "7800 cost: 1.79485117212 x= [[-0.22407498 -0.04508619  0.08614838  0.4775278  -0.29076982]\n",
      " [-0.6838508  -0.17862062 -0.46406564  0.55046201  0.77509554]\n",
      " [-0.12848629 -0.84868049 -0.54280107  1.04610549  0.47635669]\n",
      " [-0.19145619  0.84585413 -0.02414443 -0.22422453 -0.40275795]\n",
      " [ 0.04517874 -0.27188313  0.10788837  0.38481152 -0.26489133]\n",
      " [ 0.14278315 -0.22110416 -0.03250645 -0.42926273  0.54204317]\n",
      " [ 0.73106113  0.67002559  0.50930253 -1.00494835 -0.90376276]\n",
      " [ 0.3676484  -0.22641429  0.23020985 -0.10208661 -0.26983628]\n",
      " [ 0.51041427  1.27999693  0.37978526 -1.11679482 -1.05270462]\n",
      " [ 0.65331021  0.05884141  0.05659037 -0.7893787   0.0199717 ]]\n",
      "7900 cost: 1.80258190441 x= [[-0.24038508  0.03815909  0.05413769  0.46377212 -0.31226112]\n",
      " [-0.68971151 -0.14853127 -0.47561283  0.54547736  0.7674831 ]\n",
      " [-0.12820235 -0.85053371 -0.54210519  1.04632237  0.47679842]\n",
      " [-0.17094776  0.74060282  0.01606876 -0.20695524 -0.37577922]\n",
      " [ 0.0820158  -0.46038801  0.18006064  0.41578473 -0.21646408]\n",
      " [ 0.12774787 -0.14474528 -0.06178702 -0.44179956  0.5223688 ]\n",
      " [ 0.72941197  0.6782198   0.50614059 -1.00628881 -0.90594989]\n",
      " [ 0.38693923 -0.32527432  0.2680697  -0.08578589 -0.24438641]\n",
      " [ 0.48519138  1.40896075  0.33042726 -1.13801766 -1.08592473]\n",
      " [ 0.64838815  0.08413718  0.04693087 -0.79351364  0.0134497 ]]\n",
      "8000 cost: 1.91479154333 x= [[-0.25043923  0.02253002  0.05505411  0.49568427 -0.31970118]\n",
      " [-0.69332922 -0.15421126 -0.47525541  0.55707041  0.7649074 ]\n",
      " [-0.12806662 -0.85022298 -0.54214832  1.04561924  0.47690195]\n",
      " [-0.15835994  0.75995158  0.01478202 -0.24709522 -0.36654647]\n",
      " [ 0.10466284 -0.42538762  0.17778096  0.34371711 -0.19985109]\n",
      " [ 0.1185131  -0.15889209 -0.06087784 -0.41275692  0.51564487]\n",
      " [ 0.72839914  0.67672958  0.5062222  -1.00323811 -0.90671122]\n",
      " [ 0.39881412 -0.30676648  0.26684865 -0.12369277 -0.23560353]\n",
      " [ 0.46969602  1.38491649  0.3320564  -1.08876813 -1.09731864]\n",
      " [ 0.64538691  0.07951163  0.04725801 -0.78394073  0.01122877]]\n",
      "8100 cost: 1.97164538606 x= [[-0.24662541 -0.04270371  0.07381743  0.52820321 -0.30983288]\n",
      " [-0.69195699 -0.17762188 -0.46849593  0.56878866  0.76853851]\n",
      " [-0.1282175  -0.84888363 -0.54253372  1.04489638  0.47664236]\n",
      " [-0.16330911  0.84230468 -0.00911439 -0.28819904 -0.37918541]\n",
      " [ 0.09591494 -0.27786397  0.13509014  0.27009871 -0.22239702]\n",
      " [ 0.1220316  -0.21881912 -0.04356841 -0.38296285  0.52480945]\n",
      " [ 0.72874921  0.67029887  0.50804904 -1.00006239 -0.90575381]\n",
      " [ 0.39425857 -0.22957185  0.24451985 -0.16225066 -0.24732148]\n",
      " [ 0.47565151  1.28410169  0.36122751 -1.03851028 -1.08193842]\n",
      " [ 0.64659775  0.05969396  0.05300904 -0.7740803   0.01427197]]\n",
      "8200 cost: 1.89427109759 x= [[-0.22325212 -0.11390891  0.11039865  0.51153986 -0.28216497]\n",
      " [-0.68350343 -0.20321813 -0.45528001  0.56271775  0.77860055]\n",
      " [-0.12877616 -0.84738797 -0.54328614  1.04519065  0.47599958]\n",
      " [-0.19289295  0.93215024 -0.05542877 -0.26734227 -0.4142045 ]\n",
      " [ 0.04297919 -0.11684905  0.05217494  0.30758413 -0.285119  ]\n",
      " [ 0.14343374 -0.28420442 -0.01000324 -0.39807834  0.55021457]\n",
      " [ 0.73099898  0.66327939  0.51160334 -1.00166551 -0.90304559]\n",
      " [ 0.36649475 -0.14525399  0.20105359 -0.14246452 -0.28016391]\n",
      " [ 0.51183047  1.17400281  0.41790206 -1.06421038 -1.03903875]\n",
      " [ 0.65370651  0.03807061  0.06412633 -0.77905541  0.02268808]]\n",
      "8300 cost: 1.79183108386 x= [[-0.22500991 -0.0398017   0.08346468  0.47605679 -0.2923223 ]\n",
      " [-0.68412389 -0.17649692 -0.46497487  0.54994373  0.77502751]\n",
      " [-0.12879227 -0.84899224 -0.54271643  1.04589516  0.47619591]\n",
      " [-0.1907435   0.83837186 -0.02150963 -0.22254805 -0.40148541]\n",
      " [ 0.04689628 -0.28469629  0.11297903  0.38780028 -0.26227541]\n",
      " [ 0.14179346 -0.21617013 -0.03469139 -0.43061591  0.54092899]\n",
      " [ 0.73079777  0.67057972  0.50894453 -1.00516691 -0.90408529]\n",
      " [ 0.36853888 -0.23317471  0.23290207 -0.10042313 -0.26814843]\n",
      " [ 0.50914213  1.28873528  0.3763552  -1.11907578 -1.05471248]\n",
      " [ 0.65319793  0.0606128   0.05597654 -0.78982232  0.01961111]]\n",
      "8400 cost: 1.80931415694 x= [[-0.24145726  0.03905079  0.05344344  0.46459146 -0.31344646]\n",
      " [-0.69003847 -0.14801528 -0.47580772  0.54578914  0.76750168]\n",
      " [-0.12847683 -0.85073298 -0.54206904  1.04608709  0.47664497]\n",
      " [-0.1700212   0.73872706  0.01625835 -0.20814984 -0.37490864]\n",
      " [ 0.08407623 -0.46319737  0.18073661  0.4136286  -0.21460078]\n",
      " [ 0.12664545 -0.14383808 -0.06217078 -0.44106223  0.52156345]\n",
      " [ 0.72915087  0.67833906  0.50598629 -1.00628006 -0.90621846]\n",
      " [ 0.38801001 -0.326777    0.26844564 -0.08683413 -0.24312354]\n",
      " [ 0.48369431  1.41085321  0.3300125  -1.13675939 -1.08739455]\n",
      " [ 0.64821942  0.0845647   0.04690332 -0.79326715  0.01319228]]\n",
      "8500 cost: 1.92153111768 x= [[-0.25107891  0.0210335   0.05436067  0.49804817 -0.32036934]\n",
      " [-0.69350756 -0.15456887 -0.47545536  0.55794285  0.7650674 ]\n",
      " [-0.1283241  -0.85035943 -0.54211673  1.0453754   0.47675294]\n",
      " [-0.15795052  0.76116749  0.01502706 -0.25022141 -0.36628099]\n",
      " [ 0.10576742 -0.42271879  0.17853054  0.33810142 -0.19909269]\n",
      " [ 0.11782709 -0.16018656 -0.06127954 -0.41061555  0.5152944 ]\n",
      " [ 0.72819617  0.67660983  0.50607647 -1.0030769  -0.90691206]\n",
      " [ 0.3993907  -0.30540291  0.26726489 -0.126567   -0.23494068]\n",
      " [ 0.46885855  1.38307429  0.33158551 -1.08513056 -1.09801667]\n",
      " [ 0.64533901  0.0791985   0.04721636 -0.78323099  0.01112305]]\n",
      "8600 cost: 1.97007156387 x= [[-0.2450369  -0.05187832  0.07723085  0.52907717 -0.30757041]\n",
      " [-0.69133325 -0.18075413 -0.46721756  0.56910962  0.7697187 ]\n",
      " [-0.12849802 -0.84883869 -0.5425876   1.04469405  0.47644398]\n",
      " [-0.16567538  0.85329512 -0.01400073 -0.2894649  -0.38256244]\n",
      " [ 0.09201046 -0.25774226  0.12662366  0.26784165 -0.22819623]\n",
      " [ 0.12339502 -0.22718402 -0.04023023 -0.38214621  0.52711574]\n",
      " [ 0.72877544  0.66941334  0.50830916 -1.00002892 -0.90565245]\n",
      " [ 0.39220662 -0.21907294  0.2401052  -0.16334678 -0.25012514]\n",
      " [ 0.47824637  1.27032825  0.36704454 -1.03714894 -1.07813105]\n",
      " [ 0.64721049  0.05702694  0.05419533 -0.77380224  0.0150459 ]]\n",
      "8700 cost: 1.88919246116 x= [[-0.22369373 -0.10931235  0.10854985  0.50946041 -0.28333871]\n",
      " [-0.68361612 -0.20141626 -0.45589997  0.56198756  0.77850921]\n",
      " [-0.12899187 -0.84762114 -0.5432376   1.04507118  0.47588865]\n",
      " [-0.19266512  0.92577003 -0.05361104 -0.26484099 -0.41319828]\n",
      " [ 0.04369215 -0.12785426  0.05568373  0.31205579 -0.2830864 ]\n",
      " [ 0.1429453  -0.27991464 -0.01151173 -0.39999892  0.54934846]\n",
      " [ 0.73083894  0.66375141  0.51135598 -1.00192799 -0.90327208]\n",
      " [ 0.3668638  -0.15103606  0.20290665 -0.14006256 -0.27888482]\n",
      " [ 0.51127631  1.18149872  0.41553996 -1.06743    -1.04057503]\n",
      " [ 0.65369281  0.03958597  0.06370263 -0.7796884   0.02241127]]\n",
      "8800 cost: 1.78843193875 x= [[-0.22591857 -0.03406688  0.08082727  0.47448558 -0.29380534]\n",
      " [-0.68440787 -0.17429003 -0.46588436  0.54938967  0.77479451]\n",
      " [-0.12897654 -0.84924129 -0.54264984  1.04577573  0.47610564]\n",
      " [-0.18989641  0.83062056 -0.01865541 -0.2206937  -0.40004569]\n",
      " [ 0.04869142 -0.29821267  0.11833047  0.39112563 -0.25948612]\n",
      " [ 0.14088174 -0.21085168 -0.03694066 -0.43205129  0.53975558]\n",
      " [ 0.73060133  0.67115846  0.50862377 -1.00536829 -0.90433326]\n",
      " [ 0.36947495 -0.24028428  0.23572459 -0.09861653 -0.26649336]\n",
      " [ 0.50785196  1.29796155  0.37272127 -1.12149529 -1.05675622]\n",
      " [ 0.65303069  0.06245778  0.05530075 -0.79029138  0.0192319 ]]\n",
      "8900 cost: 1.81635899161 x= [[-0.24234888  0.03887372  0.05333052  0.4658364  -0.31430076]\n",
      " [-0.69031956 -0.14795863 -0.47580836  0.54625573  0.76746702]\n",
      " [-0.12864428 -0.85084212 -0.54206072  1.04592417  0.47654938]\n",
      " [-0.16917002  0.73847028  0.01596867 -0.20982981 -0.37422422]\n",
      " [ 0.08585324 -0.46330325  0.18043087  0.41061498 -0.21318575]\n",
      " [ 0.12575846 -0.14393591 -0.06211853 -0.43993049  0.52095184]\n",
      " [ 0.72896643  0.67833632  0.50591933 -1.00620604 -0.90639276]\n",
      " [ 0.38893712 -0.32684212  0.26829969 -0.08836526 -0.24220731]\n",
      " [ 0.48242347  1.41090026  0.33024807 -1.13483123 -1.0884817 ]\n",
      " [ 0.64804861  0.08461136  0.04698301 -0.79288957  0.0129996 ]]\n",
      "9000 cost: 1.92711143628 x= [[-0.25142485  0.01873176  0.05427727  0.50031184 -0.32062478]\n",
      " [-0.69359664 -0.15528375 -0.47544939  0.55877637  0.76522093]\n",
      " [-0.12848655 -0.85041629 -0.54211025  1.04520495  0.47665477]\n",
      " [-0.15776962  0.76364704  0.01473502 -0.25317964 -0.36632252]\n",
      " [ 0.10632527 -0.41796599  0.17820546  0.33280087 -0.19899082]\n",
      " [ 0.11745216 -0.16224476 -0.06121232 -0.40855192  0.51521974]\n",
      " [ 0.72807475  0.67639515  0.50601739 -1.0029001  -0.90701757]\n",
      " [ 0.3996829  -0.30292957  0.26711043 -0.1293011  -0.23472523]\n",
      " [ 0.46842472  1.37980931  0.33182924 -1.08163019 -1.09819649]\n",
      " [ 0.64532685  0.07858833  0.04729576 -0.78254504  0.01110837]]\n",
      "9100 cost: 1.96808157015 x= [[-0.24332049 -0.06091895  0.08079467  0.52973038 -0.30512385]\n",
      " [-0.69067966 -0.18389866 -0.46589896  0.56934962  0.77082382]\n",
      " [-0.12868896 -0.84874165 -0.54265569  1.04456373  0.47629632]\n",
      " [-0.16807457  0.86434322 -0.01886569 -0.29040948 -0.3859788 ]\n",
      " [ 0.08792343 -0.23768436  0.11809347  0.26617074 -0.23416078]\n",
      " [ 0.12491487 -0.23544847 -0.03683416 -0.38152612  0.5294997 ]\n",
      " [ 0.72886259  0.6685283   0.50860946 -0.9999963  -0.90548348]\n",
      " [ 0.39006229 -0.20859348  0.23565123 -0.16416111 -0.25310749]\n",
      " [ 0.48099268  1.25660454  0.37288794 -1.03612015 -1.07414879]\n",
      " [ 0.64781405  0.05435331  0.05537005 -0.77358864  0.01584495]]\n",
      "9200 cost: 1.8837453355 x= [[-0.22414868 -0.10408912  0.10657411  0.50723855 -0.28451311]\n",
      " [-0.68374875 -0.19944442 -0.45657891  0.56120492  0.77828948]\n",
      " [-0.12912334 -0.84781726 -0.54319587  1.04501598  0.47582761]\n",
      " [-0.19230265  0.91880398 -0.05144016 -0.26211855 -0.41201531]\n",
      " [ 0.04453358 -0.14005995  0.05973178  0.31693092 -0.28082332]\n",
      " [ 0.14247982 -0.27506986 -0.0132094  -0.4020463   0.54839939]\n",
      " [ 0.73072169  0.66427426  0.51111956 -1.00218564 -0.90345413]\n",
      " [ 0.36730302 -0.15743368  0.20503668 -0.13747354 -0.27756826]\n",
      " [ 0.51065886  1.18982043  0.41279411 -1.0708629  -1.0422129 ]\n",
      " [ 0.65363131  0.04124898  0.0631884  -0.78036328  0.02210607]]\n",
      "9300 cost: 1.78345338067 x= [[-0.22674065 -0.02766666  0.07804961  0.47255859 -0.29523057]\n",
      " [-0.68467472 -0.17189199 -0.46685753  0.54870443  0.77446604]\n",
      " [-0.12908605 -0.84945946 -0.54258761  1.04571943  0.47605993]\n",
      " [-0.18905103  0.82222172 -0.01544991 -0.21835441 -0.3985189 ]\n",
      " [ 0.05038104 -0.31303226  0.12422786  0.39532813 -0.25661871]\n",
      " [ 0.14008107 -0.20494491 -0.0393839  -0.43380685  0.53856058]\n",
      " [ 0.7304534   0.67179264  0.5083106  -1.0055863  -0.90453557]\n",
      " [ 0.37035835 -0.24806918  0.23882937 -0.09637004 -0.26487257]\n",
      " [ 0.50665603  1.30808702  0.36869909 -1.12445849 -1.05880307]\n",
      " [ 0.65284971  0.06446425  0.05453533 -0.79086588  0.01884429]]\n",
      "9400 cost: 1.82353995572 x= [[-0.24308545  0.03774731  0.05366557  0.46744766 -0.31488833]\n",
      " [-0.69055801 -0.14829175 -0.47565865  0.54685362  0.76742288]\n",
      " [-0.12874577 -0.85088809 -0.54206936  1.04580478  0.47648903]\n",
      " [-0.16841663  0.73958128  0.01527462 -0.21193396 -0.37373081]\n",
      " [ 0.08736174 -0.46108146  0.1793211   0.40684433 -0.21218427]\n",
      " [ 0.12504325 -0.14492097 -0.06171537 -0.43846303  0.52051854]\n",
      " [ 0.72883402  0.6782322   0.50591624 -1.00608106 -0.90650407]\n",
      " [ 0.38972614 -0.32567478  0.26772645 -0.0903153  -0.2415759 ]\n",
      " [ 0.481357    1.40935794  0.33102297 -1.13233386 -1.08923901]\n",
      " [ 0.64788883  0.08433499  0.04715565 -0.79240168  0.01286476]]\n",
      "9500 cost: 1.93144676323 x= [[-0.25151852  0.01581496  0.05469324  0.50236256 -0.32054183]\n",
      " [-0.69360593 -0.15626291 -0.47527361  0.55952874  0.76540176]\n",
      " [-0.12859096 -0.85042003 -0.54211948  1.0450831   0.4765871 ]\n",
      " [-0.157815    0.76705905  0.01395995 -0.25584036 -0.36665584]\n",
      " [ 0.10639066 -0.41165549  0.17694272  0.32803966 -0.19947863]\n",
      " [ 0.11733225 -0.16488484 -0.06074351 -0.40667365  0.51539235]\n",
      " [ 0.72801073  0.676112    0.50602464 -1.00272678 -0.90705745]\n",
      " [ 0.39971731 -0.29962993  0.26645885 -0.13176735 -0.23488251]\n",
      " [ 0.46834676  1.37548078  0.33270309 -1.07844961 -1.09793019]\n",
      " [ 0.64535701  0.07775832  0.04748658 -0.78191967  0.01117386]]\n",
      "9600 cost: 1.96523021043 x= [[-0.2414889  -0.0695336   0.08437658  0.52995828 -0.30257172]\n",
      " [-0.68999583 -0.18692851 -0.46458407  0.56943439  0.77188032]\n",
      " [-0.12882464 -0.84861904 -0.54272933  1.04448535  0.47618092]\n",
      " [-0.17053681  0.87499585 -0.02361995 -0.29078507 -0.38940717]\n",
      " [ 0.08364592 -0.21843954  0.10969582  0.26552182 -0.24020567]\n",
      " [ 0.12656364 -0.2433367  -0.03347055 -0.38129569  0.5319255 ]\n",
      " [ 0.72899127  0.66767953  0.50892838 -0.99999267 -0.90527465]\n",
      " [ 0.38781997 -0.19853189  0.23126216 -0.16445703 -0.25618792]\n",
      " [ 0.48388623  1.24344153  0.37863223 -1.03574864 -1.07007351]\n",
      " [ 0.64842223  0.05177995  0.05651486 -0.77350476  0.01665623]]\n",
      "9700 cost: 1.87789266651 x= [[-0.22469633 -0.09797565  0.10434415  0.50482166 -0.28581697]\n",
      " [-0.68392478 -0.19718779 -0.45735893  0.56034934  0.77794514]\n",
      " [-0.12920022 -0.84799994 -0.54315248  1.04500237  0.47580106]\n",
      " [-0.19174643  0.91084197 -0.04882605 -0.25912115 -0.41055719]\n",
      " [ 0.04565032 -0.15414319  0.06451387  0.32230025 -0.27812169]\n",
      " [ 0.14194889 -0.26942121 -0.01518596 -0.40426926  0.5472805 ]\n",
      " [ 0.73062262  0.66487963  0.51087316 -1.00244928 -0.90362285]\n",
      " [ 0.36788718 -0.16480654  0.20754695 -0.13464064 -0.2760735 ]\n",
      " [ 0.50986899  1.1994287   0.4095381  -1.07459359 -1.04411624]\n",
      " [ 0.6535135   0.04315593  0.06256424 -0.78109659  0.02174274]]\n",
      "9800 cost: 1.77704546534 x= [[-0.22748064 -0.02081049  0.07514098  0.47037069 -0.29660197]\n",
      " [-0.68492151 -0.16935974 -0.46788751  0.54792113  0.77408584]\n",
      " [-0.12914926 -0.84965898 -0.54252519  1.04570318  0.47604223]\n",
      " [-0.18823997  0.81337059 -0.01196946 -0.2156617  -0.3969592 ]\n",
      " [ 0.0519446  -0.32875399  0.13056494  0.40016763 -0.25374082]\n",
      " [ 0.13937393 -0.19863616 -0.04198756 -0.43579694  0.5373693 ]\n",
      " [ 0.73033833  0.67246668  0.5079984  -1.00581727 -0.90470897]\n",
      " [ 0.37117686 -0.25632451  0.24216196 -0.09380113 -0.26329228]\n",
      " [ 0.50556025  1.31883615  0.36436766 -1.12782254 -1.0608264 ]\n",
      " [ 0.65266809  0.06658315  0.05370084 -0.79151794  0.01845603]]\n",
      "9900 cost: 1.83070956204 x= [[-0.243706    0.0360171   0.05423433  0.46931516 -0.31529528]\n",
      " [-0.69076383 -0.14887302 -0.47543271  0.54754272  0.76737898]\n",
      " [-0.12880641 -0.85089413 -0.54208636  1.04571271  0.47645069]\n",
      " [-0.1677471   0.74156092  0.01438755 -0.21433355 -0.37337408]\n",
      " [ 0.08866152 -0.45738033  0.17781409  0.40254317 -0.21147179]\n",
      " [ 0.12445224 -0.14647044 -0.06113471 -0.43676339  0.52021108]\n",
      " [ 0.72873605  0.67806541  0.50594924 -1.00592178 -0.90657561]\n",
      " [ 0.39040736 -0.32372465  0.26694056 -0.09255837 -0.24113719]\n",
      " [ 0.48044645  1.40680619  0.33206475 -1.12944238 -1.0897698 ]\n",
      " [ 0.64774158  0.0838539   0.04737253 -0.79183832  0.01276993]]\n",
      "10000 cost: 1.93570979631 x= [[-0.25156059  0.01266412  0.05530217  0.50446011 -0.32034916]\n",
      " [-0.6936047  -0.15735283 -0.47503625  0.56029493  0.76556373]\n",
      " [-0.12865683 -0.85039431 -0.54213476  1.04498902  0.47654097]\n",
      " [-0.15786712  0.77086649  0.01303309 -0.25853771 -0.36704353]\n",
      " [ 0.10638986 -0.40470961  0.1753628   0.32321381 -0.20010455]\n",
      " [ 0.11727433 -0.16775187 -0.06013109 -0.40475036  0.5156284 ]\n",
      " [ 0.72797241  0.67580239  0.50606255 -1.00253874 -0.90706711]\n",
      " [ 0.39971764 -0.29599321  0.26563802 -0.13427831 -0.2351502 ]\n",
      " [ 0.46832612  1.37072132  0.33378792 -1.0751958  -1.09754341]\n",
      " [ 0.64538154  0.07683649  0.04771163 -0.78127992  0.01125852]]\n",
      "Dev precision (%): 25.522252\n"
     ]
    }
   ],
   "source": [
    "# Try different regularizations and pick the best!\n",
    "\n",
    "### YOUR CODE HERE\n",
    "\n",
    "regularization = 0.0003 # try 0.0, 0.00001, 0.00003, 0.0001, 0.0003, 0.001, 0.003, 0.01 and pick the best\n",
    "\n",
    "### END YOUR CODE\n",
    "\n",
    "random.seed(3141)\n",
    "np.random.seed(59265)\n",
    "weights = np.random.randn(dimVectors, 5)\n",
    "\n",
    "trainset = dataset.getTrainSentences()\n",
    "nTrain = len(trainset)\n",
    "trainFeatures = np.zeros((nTrain, dimVectors))\n",
    "trainLabels = np.zeros((nTrain,), dtype=np.int32)\n",
    "\n",
    "for i in xrange(nTrain):\n",
    "    words, trainLabels[i] = trainset[i]\n",
    "    trainFeatures[i, :] = getSentenceFeature(tokens, wordVectors, words)\n",
    "    \n",
    "# We will do batch optimization\n",
    "weights = sgd(lambda weights: softmax_wrapper(trainFeatures, trainLabels, weights, regularization), weights, 3.0, 10000, PRINT_EVERY=100)\n",
    "\n",
    "# Prepare dev set features\n",
    "devset = dataset.getDevSentences()\n",
    "nDev = len(devset)\n",
    "devFeatures = np.zeros((nDev, dimVectors))\n",
    "devLabels = np.zeros((nDev,), dtype=np.int32)\n",
    "\n",
    "for i in xrange(nDev):\n",
    "    words, devLabels[i] = devset[i]\n",
    "    devFeatures[i, :] = getSentenceFeature(tokens, wordVectors, words)\n",
    "    \n",
    "_, _, pred = softmaxRegression(devFeatures, devLabels, weights)\n",
    "print \"Dev precision (%%): %f\" % precision(devLabels, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Write down the best regularization and accuracy you found\n",
    "# sanity check: your accuracy should be around or above 30%\n",
    "\n",
    "### YOUR CODE HERE\n",
    "\n",
    "BEST_REGULARIZATION = 1\n",
    "BEST_ACCURACY = 0.0\n",
    "\n",
    "### END YOUR CODE\n",
    "\n",
    "print \"=== For autograder ===\\n%g\\t%g\" % (BEST_REGULARIZATION, BEST_ACCURACY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== For autograder ===\n",
      "Test precision (%): 23.031674\n"
     ]
    }
   ],
   "source": [
    "# Test your findings on the test set\n",
    "\n",
    "testset = dataset.getTestSentences()\n",
    "nTest = len(testset)\n",
    "testFeatures = np.zeros((nTest, dimVectors))\n",
    "testLabels = np.zeros((nTest,), dtype=np.int32)\n",
    "\n",
    "for i in xrange(nTest):\n",
    "    words, testLabels[i] = testset[i]\n",
    "    testFeatures[i, :] = getSentenceFeature(tokens, wordVectors, words)\n",
    "    \n",
    "_, _, pred = softmaxRegression(testFeatures, testLabels, weights)\n",
    "print \"=== For autograder ===\\nTest precision (%%): %f\" % precision(testLabels, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extra Credit\n",
    "\n",
    "Train your own classifier for sentiment analysis! We will not provide any starter code for this part, but you can feel free to reuse the code you've written before, or write some new code for this task. Also feel free to refer to the code we provided you with to see how we scaffolded training for you.\n",
    "\n",
    "Try to contain all of your code in one code block. You could start by using multiple blocks, then paste code together and remove unnecessary blocks. Report, as the last two lines of the output of your block, the dev set accuracy and test set accuracy you achieved, in the format we used above.\n",
    "\n",
    "*Note: no credits will be given for this part if you use the dev or test sets for training, or if you fine-tune your regularization or other hyperparameters on the test set.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== For autograder ===\n",
      "Dev precision (%): 25.522252\n",
      "Test precision (%): 23.031674\n"
     ]
    }
   ],
   "source": [
    "### YOUR CODE HERE\n",
    "\n",
    "### END YOU CODE\n",
    "\n",
    "\n",
    "_, _, pred = softmaxRegression(devFeatures, devLabels, weights)\n",
    "print \"=== For autograder ===\\nDev precision (%%): %f\" % precision(devLabels, pred)\n",
    "_, _, pred = softmaxRegression(testFeatures, testLabels, weights)\n",
    "print \"Test precision (%%): %f\" % precision(testLabels, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
